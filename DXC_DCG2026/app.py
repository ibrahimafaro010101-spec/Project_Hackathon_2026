# ============================================================
# app.py ‚Äî LIK Insurance Analyst
# Orchestrateur central de tous les moteurs
# ============================================================


# Importation des libraries necessaires
# IMPORTS DES BIBLIOTH√àQUES STANDARD ET TIERS

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import os
import sys
import json
import io
import tempfile
import zipfile
import traceback
import warnings
from datetime import datetime
from dotenv import load_dotenv
# Import additionnel pour les m√©triques d'√©valuation des mod√®les et les sous-figures Plotly
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error
from plotly.subplots import make_subplots

#Ignorer les warnings pour une sortie console plus propre
warnings.filterwarnings('ignore')

# Import des modules personnalis√©s de l'application
import streamlit as st
from modules.report_engine import ReportEngine
from modules.custom_llm_client import OpenAIAnalyzer

# ========== INITIALISATION SESSION STATE ==========

#INITIALISATION DES VARIABLES D'√âTAT DE LA SESSION#

# Ces variables persistent lors de la navigation dans l'application Streamlit.#
if 'openai_client' not in st.session_state:
    st.session_state.openai_client = None

if 'report_engine' not in st.session_state:
    st.session_state.report_engine = None



# ------------------------------------------------------------
# CHARGEMENT DES VARIABLES D'ENVIRONNEMENT
#Tente de charger la cl√© API OpenAI depuis un fichier .env
#ou les variables d'environnement syst√®me.
# ------------------------------------------------------------
OPENAI_API_KEY = ""

# Essayer de charger depuis .env avec plusieurs encodages
encodings_to_try = ['utf-8', 'latin-1', 'utf-16', 'cp1252']

for encoding in encodings_to_try:
    try:
        # R√©initialiser dotenv
        from dotenv import dotenv_values
        env_vars = dotenv_values(".env", encoding=encoding)
        OPENAI_API_KEY = env_vars.get("OPENAI_API_KEY", "")
        if OPENAI_API_KEY:
            print(f" Fichier .env charg√© avec encodage: {encoding}")
            break
    except:
        continue

# Si .env √©choue, essayer depuis les variables syst√®me
if not OPENAI_API_KEY:
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")

# Si toujours rien, laisser vide
if not OPENAI_API_KEY:
    print(" Aucune cl√© API trouv√©e dans .env ou variables syst√®me")
# ------------------------------------------------------------
# D√âFINITION DES CHEMINS D'ACC√àS
# Configure les chemins vers le r√©pertoire de base et les assets.
# ------------------------------------------------------------
BASE_DIR = os.path.dirname(__file__)
sys.path.append(os.path.join(BASE_DIR, "modules"))
ASSETS_DIR = os.path.join(BASE_DIR, "assets")

# V√©rifier si report_engine est disponible
try:
    from report_engine import ReportEngine

    REPORT_ENGINE_AVAILABLE = True
except ImportError:
    REPORT_ENGINE_AVAILABLE = False
    ReportEngine = None


# ------------------------------------------------------------
# CONFIGURATION DE LA PAGE STREAMLIT
# D√©finit le titre, l'ic√¥ne, la mise en page et l'√©tat initial de la barre lat√©rale.
# ------------------------------------------------------------
st.set_page_config(
    page_title="LIK Insurance Analyst",
    page_icon="",
    layout="wide",
    initial_sidebar_state="expanded"
)



# ------------------------------------------------------------
# CHARGEMENT DU FICHIER CSS
# Applique des styles personnalis√©s pour l'interface utilisateur.
# ------------------------------------------------------------
def load_css():
    css_path = os.path.join(ASSETS_DIR, "style.css")
    if os.path.exists(css_path):
        with open(css_path) as f:
            st.markdown(f"<style>{f.read()}</style>", unsafe_allow_html=True)
    else:
        # Style CSS par d√©faut
        st.markdown("""
        <style>
        .main-header {
            color: #1E3A8A;
            text-align: center;
            font-size: 2.5rem;
            margin-bottom: 1rem;
        }
        .stButton > button {
            width: 100%;
        }
        </style>
        """, unsafe_allow_html=True)


load_css()

# ------------------------------------------------------------
# √âTAT DE LA SESSION (CENTRAL)
# Initialise toutes les variables d'√©tat de session avec des valeurs par d√©faut.
# Ces variables stockent les donn√©es et l'√©tat de l'application entre les interactions.
# ------------------------------------------------------------
session_defaults = {
    "df": None,
    "dataframe": None,
    "metadata": None,
    "data_ready": False,
    "data_loaded": False,
    "metadata_ready": False,
    "openai_client": None,
    "nlq_engine": None,
    "predictive_engine": None,
    "insight_engine": None,
    "scored_clients": None,
    "client_table": None,
    "raw_data": None,
    "conversation_history": [],
    "business_context": None,
    "column_documentation": None,
    "column_explainer": None,
    "uploaded_file_name": None,
    "df_final": None,
    "report_engine": None,
    "using_mateur": False,
    "generated_report_md": None,
    "generated_report_pdf": None,
    "generated_report_word": None,
    "generated_report_html": None,
    "data_processor": None,
    "df_processed": None,
    "df_enriched": None,
    "df_segmented": None,
    "pipeline_results": None,
    "data_processed": False,
    "predictive_engine": None,
    "df_prepared": None,
    "time_series_data": None,
    'df_final' : None
}
# Initialisation : pour chaque cl√©,
# si elle n'existe pas dans session_state, l'ajouter avec sa valeur par d√©faut.

for key, value in session_defaults.items():
    if key not in st.session_state:
        st.session_state[key] = value

# ------------------------------------------------------------
# EN-T√äTE DE L'APPLICATION
# Affiche le titre principal et la description.
# ------------------------------------------------------------
st.markdown('<h1 class="main-header">LIK Insurance Analyst</h1>', unsafe_allow_html=True)
st.markdown('<p style="font-size: 16px;">Transformer vos donn√©es en d√©cisions viables</p>', unsafe_allow_html=True)
# ============================================================
# BARRE LAT√âRALE (SIDEBAR)
# Contient la navigation, la configuration API, l'√©tat de l'application.
# ============================================================
with st.sidebar:
    # Logo centr√©
    logo_path = os.path.join(ASSETS_DIR, "logo0.png")
    if os.path.exists(logo_path):
        col1, col2, col3 = st.columns([1, 2, 1])
        with col2:
            st.image(logo_path, width=300)

        st.markdown(
            """
            <div style="text-align: center;">
                <p style="margin-top: 5px; margin-bottom: 20px; font-size: 14px;">
                    <b>Livrer l'excellence</b>
                </p>
            </div>
            """,
            unsafe_allow_html=True
        )
    else:
        st.markdown("###  LIK Insurance")

    # Section API masqu√©e - gard√©e uniquement pour la logique backend
    if st.session_state.get('show_api_config', False):  # Condition pour afficher/masquer
        st.markdown("###  Configuration API")

        api_key_input = st.text_input(
            "Cl√© OpenAI API",
            type="password",
            value=OPENAI_API_KEY,
            help="Obtenez votre cl√© sur platform.openai.com",
            placeholder="sk-...",
            key="api_key_input"
        )
    else:
        # Utiliser la cl√© API sans l'afficher dans l'interface
        api_key_input = OPENAI_API_KEY  # Utiliser directement la valeur du .env

    # Initialiser les clients OpenAI
    # Dans la sidebar de app.py, modifiez cette partie :
    if api_key_input:
        try:
            # from llm_client import OpenAIAnalyzer  # ‚Üê COMMENTEZ CETTE LIGNE
            from custom_llm_client import OpenAIAnalyzer  # ‚Üê UTILISEZ CE NOUVEAU NOM

            # V√©rifier que la cl√© n'est pas vide
            if api_key_input.strip():
                st.session_state.openai_client = OpenAIAnalyzer(api_key=api_key_input)
                st.success(" Connexion valide")
            else:
                st.warning(" Veuillez entrer une cl√© API valide")
                st.session_state.openai_client = None

        except ImportError as e:
            st.error(f" Module custom_llm_client non disponible: {e}")
        except ValueError as e:
            st.error(f" Cl√© API invalide: {e}")
        except Exception as e:
            st.error(f" Erreur d'initialisation: {e}")
    else:
        if 'openai_client' in st.session_state:
            st.session_state.openai_client = None
            st.session_state.nlq_engine = None
            st.session_state.report_engine = None

# Navigation
    from streamlit import container

    # Barre horizontale avec espace r√©duit
    st.markdown("<hr style='margin: 2px 0 10px 0;'>", unsafe_allow_html=True)

    # Navigation dans un container
    with container():
        st.markdown("<h3 style='text-align: center; margin: 0 0 13px 0; padding: 0;'>Navigation</h3>",
                    unsafe_allow_html=True)
    page = st.radio(
        "",
        [
            " üì§ Chargement des donn√©es",
            " üè∑Ô∏è M√©tadonn√©es",
            " üîÑ Traitement des donn√©es",
            " üí¨ Assistant IA",
            " üëÅÔ∏è Visualisation des donn√©es",
            " üßÆ Mod√®les Pr√©dictifs",
            " üìÑ Rapport Intelligent",
            "üè¢ √Ä Propos"

        ]
    )

    st.markdown("---")

    def ensure_metadata_available():
        """
        V√©rifie que les m√©tadonn√©es sont disponibles, sinon les g√©n√®re
        """
        if not st.session_state.metadata_ready and st.session_state.data_loaded:
            try:
                from modules.metadata_extractor import MetadataExtractor
                from modules.business_context import BusinessContextProvider

                df = st.session_state.dataframe

                with st.spinner("üîç G√©n√©ration des m√©tadonn√©es en cours..."):
                    metadata_extractor = MetadataExtractor(df)
                    metadata = metadata_extractor.extract_safe_metadata()

                    business_context = BusinessContextProvider.get_context(
                        BusinessContextProvider.infer_domain_from_columns(df.columns)
                    )

                    st.session_state.metadata = metadata
                    st.session_state.business_context = business_context
                    st.session_state.metadata_ready = True

                st.success("‚úÖ M√©tadonn√©es g√©n√©r√©es avec succ√®s")
                return True

            except ImportError:
                st.warning("‚ö†Ô∏è Impossible de g√©n√©rer les m√©tadonn√©es automatiquement")
                return False
        return st.session_state.metadata_ready


    # √âtat de l'application
    st.markdown("###  √âtat")
    if st.session_state.data_loaded:
        df = st.session_state.dataframe
        st.success(" Donn√©es charg√©es")
        st.caption(f"‚Ä¢ {len(df):,} lignes")
        st.caption(f"‚Ä¢ {len(df.columns)} colonnes")
    else:
        st.warning(" Aucune donn√©e")

    if st.session_state.metadata is not None:
        st.success(" M√©tadonn√©es pr√™tes")

    if st.session_state.scored_clients is not None:
        st.success(" Analyse risque compl√®te")

# ============================================================
# 1Ô∏è‚É£ PAGE : CHARGEMENT DES DONN√âES
# Permet √† l'utilisateur de t√©l√©verser un fichier de donn√©es.
# ============================================================
if page == " üì§ Chargement des donn√©es":
    st.header(" Chargement des donn√©es")
    col1, col2 = st.columns([2, 1])

    with col1:
        uploaded_file = st.file_uploader(
            "T√©l√©versez votre fichier de donn√©es",
            type=["csv", "xlsx", "xls", "txt", "dta"],
            help="Formats support√©s: CSV, Excel, Texte, Stata"
        )

        if uploaded_file is not None:
            try:
                # Sauvegarder le nom du fichier
                st.session_state.uploaded_file_name = uploaded_file.name

                # D√©tection du type de fichier
                file_name = uploaded_file.name.lower()

                if file_name.endswith('.csv'):
                    # Essayer diff√©rents encodages
                    encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']
                    df = None
                    for encoding in encodings:
                        try:
                            uploaded_file.seek(0)  # R√©initialiser le pointeur
                            df = pd.read_csv(uploaded_file, encoding=encoding)
                            break
                        except UnicodeDecodeError:
                            continue
                    if df is None:
                        # Dernier essai avec erreurs ignor√©es
                        uploaded_file.seek(0)
                        df = pd.read_csv(uploaded_file, encoding='utf-8', errors='replace')
                elif file_name.endswith(('.xlsx', '.xls')):
                    df = pd.read_excel(uploaded_file)
                elif file_name.endswith('.txt'):
                    df = pd.read_csv(uploaded_file, sep='\t', encoding='utf-8')
                elif file_name.endswith('.dta'):
                    df = pd.read_stata(uploaded_file)
                else:
                    st.error("Format de fichier non support√©")
                    df = None

                if df is not None:
                    # Pr√©paration des donn√©es
                    try:
                        from data_prep_engine import DataPrepEngine

                        prep = DataPrepEngine()
                        df = prep.clean_data(df)
                        df = prep.engineer_features(df)
                        st.session_state.df_final = df
                    except ImportError:
                        # Si le module n'est pas disponible, utiliser les donn√©es brutes
                        st.info("Module data_prep_engine non disponible - donn√©es brutes utilis√©es")
                        st.session_state.df_final = df

                    # Affichage des informations de base
                    st.success(f" Fichier charg√©: {uploaded_file.name}")

                    with st.expander(" Aper√ßu des donn√©es", expanded=True):
                        st.dataframe(df.head(5), use_container_width=True) # On charge les 5 1eres observations

                    # Statistiques rapides
                    col_stat1, col_stat2, col_stat3, col_stat4 = st.columns(4)
                    with col_stat1:
                        st.metric("Lignes", f"{len(df):,}")
                    with col_stat2:
                        st.metric("Colonnes", len(df.columns))
                    with col_stat3:
                        st.metric("Valeurs manquantes", f"{df.isna().sum().sum():,}")
                    with col_stat4:
                        st.metric("Doublons", f"{df.duplicated().sum():,}")

                    # Sauvegarde dans la session
                    st.session_state.dataframe = df
                    st.session_state.df = df
                    st.session_state.data_loaded = True
                    st.session_state.data_ready = True

                    # R√©initialiser les analyses existantes
                    st.session_state.metadata = None
                    st.session_state.business_context = None
                    st.session_state.scored_clients = None
                    st.session_state.client_table = None

                    st.success(" Donn√©es pr√™tes pour l'analyse!")

            except Exception as e:
                st.error(f" Erreur lors du chargement: {str(e)}")
                st.code(traceback.format_exc())

    with col2:
        st.markdown("**100% de S√©curit√©**")
        st.info("""
        Cette application vous permet de capitaliser sur vos objectifs pour le profilage et la gestion des clients en risque.

        **Vos donn√©es restent:**
        - En local sur votre ordinateur
        - Ne sont jamais partag√©es
        - Enti√®rement sous votre contr√¥le
        """)
        # Je pense que c'est bien

# ============================================================
#  PAGE : M√âTADONN√âES
# Extrait et affiche les m√©tadonn√©es des donn√©es charg√©es (structure, types, contexte m√©tier).
# ============================================================
elif page == " üè∑Ô∏è M√©tadonn√©es":
    st.header(" Extraction des M√©tadonn√©es")

    if not st.session_state.data_loaded:
        st.warning(" Veuillez d'abord charger des donn√©es")
        st.stop()

    df = st.session_state.dataframe

    try:
        from metadata_extractor import MetadataExtractor
        from business_context import BusinessContextProvider

        with st.spinner("Extraction des m√©tadonn√©es s√©curis√©es..."):
            metadata_extractor = MetadataExtractor(df)
            metadata = metadata_extractor.extract_safe_metadata()
            schema_json = metadata_extractor.generate_schema_json()

            business_context = BusinessContextProvider.get_context(
                BusinessContextProvider.infer_domain_from_columns(df.columns)
            )

            st.session_state.metadata = metadata
            st.session_state.business_context = business_context

        st.success(" M√©tadonn√©es extraites avec succ√®s!")

        tab1, tab2, tab3 = st.tabs([" Vue d'ensemble", " Structure", " Contexte M√©tier"])

        with tab1:
            st.subheader("Informations G√©n√©rales")
            general_info = metadata.get('general_info', {})

            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("Lignes", general_info.get('nombre_lignes', 0))
            with col2:
                st.metric("Colonnes", general_info.get('nombre_colonnes', 0))
            with col3:
                st.metric("M√©moire (Mo)", f"{general_info.get('taille_memoire_mo', 0):.1f}")
            with col4:
                st.metric("Qualit√©", f"{metadata.get('quality_indicators', {}).get('completude_pct', 0):.1f}%")

            st.subheader("Types de Donn√©es")
            dtype_summary = metadata.get('data_types_summary', {})
            if dtype_summary:
                dtype_df = pd.DataFrame({
                    "Type": list(dtype_summary.keys()),
                    "Nombre": list(dtype_summary.values())
                })
                st.dataframe(dtype_df, use_container_width=True)

        with tab2:
            st.subheader("Structure des Colonnes")
            columns_info = metadata.get('structure_columns', [])
            columns_df = pd.DataFrame(columns_info[:10])
            st.dataframe(columns_df, use_container_width=True)

            st.subheader("Profils Statistiques")
            profiles = metadata.get('statistical_profiles', {})

            if profiles.get('variables_numeriques'):
                st.markdown("**Variables Num√©riques:**")
                for var in profiles['variables_numeriques'][:3]:
                    st.markdown(f"- {var['nom']}: [{var['plage']['min']:.2f}, {var['plage']['max']:.2f}]")

            if profiles.get('variables_categorielles'):
                st.markdown("**Variables Cat√©gorielles:**")
                for var in profiles['variables_categorielles'][:3]:
                    st.markdown(f"- {var['nom']}: {var['categories_count']} cat√©gories")

        with tab3:
            st.subheader("Contexte M√©tier Inf√©r√©")
            context = business_context

            st.markdown(f"**Domaine:** {context.get('domaine', 'Non d√©termin√©')}")
            st.markdown(f"**Description:** {context.get('description', '')}")

            st.markdown("**Concepts Cl√©s:**")
            concepts = context.get('concepts_cles', [])
            for concept in concepts[:5]:
                st.markdown(f"- {concept}")

            st.markdown("**Analyses Courantes:**")
            analyses = context.get('analyses_courantes', [])
            for analyse in analyses[:3]:
                st.markdown(f"- {analyse}")

        st.markdown("---")
        st.subheader(" Export des M√©tadonn√©es")
        col_exp1, col_exp2 = st.columns(2)

        with col_exp1:
            json_str = json.dumps(schema_json, indent=2, ensure_ascii=False)
            st.download_button(
                label=" T√©l√©charger JSON",
                data=json_str,
                file_name="metadata.json",
                mime="application/json"
            )

        with col_exp2:
            context_str = json.dumps(context, indent=2, ensure_ascii=False)
            st.download_button(
                label=" Contexte M√©tier",
                data=context_str,
                file_name="business_context.json",
                mime="application/json"
            )

    except ImportError as e:
        st.error(f" Erreur d'importation: {e}")
        st.info("V√©rifiez que les modules s√©curis√©s sont install√©s dans le dossier 'modules/'")

# ============================================================
# PAGE : TRAITEMENT DES DONN√âES
# Offre des outils avanc√©s pour le nettoyage, l'analyse scientifique et la pr√©paration des donn√©es.
# ============================================================

elif page == " üîÑ Traitement des donn√©es":
    st.header("üîÑ Traitement De Donn√©es")

    if not st.session_state.data_loaded:
        st.warning("‚ö†Ô∏è Veuillez d'abord charger des donn√©es")
        st.stop()

    # Utiliser les donn√©es finales si disponibles, sinon les donn√©es brutes
    if st.session_state.df_final is not None:
        df = st.session_state.df_final.copy()
    else:
        df = st.session_state.dataframe.copy()

    # Initialisation du moteur de traitement scientifique
    try:
        from modules.data_processing_engine import DataProcessingEngine

        # CORRECTION : Initialiser le moteur avec une v√©rification correcte
        if 'data_processor' not in st.session_state or st.session_state.data_processor is None:
            st.session_state.data_processor = DataProcessingEngine()
            st.success("‚úÖ Vous pouvez commencer vos traitements")

        processor = st.session_state.data_processor

    except ImportError as e:
        st.error(f"‚ùå Module data_processing_engine non disponible: {e}")
        st.info("""
        **Assurez-vous que:**
        1. Le fichier `data_processing_engine.py` est dans le dossier `modules/`
        2. La classe `DataProcessingEngine` est bien d√©finie
        """)
        st.stop()

    # Le reste du code reste inchang√©...

    # Onglets de traitement
    tab1, tab2, tab3, tab4 = st.tabs([
        "üß¨ Analyse Scientifique",
        "üîß Pr√©traitement",
        "üéØ D√©tection Cibles",
        "üìä Statistiques"
    ])

    # ============================================================
    # TAB 1: ANALYSE SCIENTIFIQUE
    # D√©tection automatique des types de variables et analyse statistique.
    # ============================================================
    with tab1:
        st.subheader("üß¨ Analyse Scientifique des Donn√©es")

        col1, col2 = st.columns([2, 1])

        with col1:
            st.markdown("""
            **üîç D√©tection scientifique des types:**
            - Identification automatique des types de variables
            - Tests statistiques rigoureux
            - Analyse de distribution approfondie
            - D√©tection d'anomalies scientifiques
            """)

            if st.button("üî¨ Ex√©cuter l'analyse scientifique", type="primary", use_container_width=True):
                with st.spinner("üß¨ Analyse scientifique en cours..."):
                    try:
                        # D√©tection des types de colonnes
                        column_types = processor.detect_column_types(df)

                        # Afficher le r√©sum√© statistique
                        statistical_summary = processor.get_statistical_summary()

                        st.success("‚úÖ Analyse scientifique termin√©e!")

                        # Afficher les types de variables
                        st.markdown("#### üìä Types de variables d√©tect√©s")

                        type_counts = statistical_summary.get("variable_types", {})
                        if type_counts:
                            type_df = pd.DataFrame({
                                "Type de variable": list(type_counts.keys()),
                                "Nombre": list(type_counts.values())
                            })
                            st.dataframe(type_df, use_container_width=True)

                        # M√©triques de qualit√©
                        st.markdown("#### üéØ M√©triques de qualit√©")

                        quality_metrics = statistical_summary.get("data_quality", {})
                        col_q1, col_q2, col_q3, col_q4 = st.columns(4)

                        with col_q1:
                            st.metric("Variables totales", quality_metrics.get("total_variables", 0))

                        with col_q2:
                            complete = quality_metrics.get("complete_variables", 0)
                            total = quality_metrics.get("total_variables", 1)
                            percentage = (complete / total * 100) if total > 0 else 0
                            st.metric("Variables compl√®tes", f"{complete} ({percentage:.1f}%)")

                        with col_q3:
                            st.metric("Variables normales", quality_metrics.get("normal_variables", 0))

                        with col_q4:
                            st.metric("Haute qualit√©", quality_metrics.get("high_quality_variables", 0))

                        # D√©tails par colonne
                        with st.expander("üìÑ D√©tails par colonne", expanded=False):
                            for col, info in list(column_types.items())[:10]:  # Limiter √† 10 colonnes
                                with st.expander(f"Colonne: {col}", expanded=False):
                                    col1_info, col2_info = st.columns(2)

                                    with col1_info:
                                        st.markdown(f"**Type:** {info.get('type', 'N/A')}")
                                        st.markdown(f"**Type original:** {info.get('original_dtype', 'N/A')}")
                                        st.markdown(f"**Valeurs uniques:** {info.get('unique_values', 0)}")
                                        st.markdown(f"**Valeurs manquantes:** {info.get('missing_percentage', 0):.1f}%")

                                    with col2_info:
                                        if 'distribution' in info:
                                            dist = info['distribution']
                                            if 'mean' in dist:
                                                st.markdown(f"**Moyenne:** {dist['mean']:.2f}")
                                                st.markdown(f"**√âcart-type:** {dist['std']:.2f}")
                                            if 'skewness' in dist:
                                                st.markdown(f"**Asym√©trie:** {dist['skewness']:.2f}")

                        if len(column_types) > 10:
                            st.info(f"... et {len(column_types) - 10} autres colonnes analys√©es")

                    except Exception as e:
                        st.error(f"‚ùå Erreur lors de l'analyse: {str(e)}")
                        st.code(traceback.format_exc())

        with col2:
            st.info("""
            **üéØ M√©thodologie :**

            1. **Tests statistiques** (Shapiro-Wilk, Anderson-Darling)
            2. **Analyse de distribution** compl√®te
            3. **D√©tection d'anomalies** avec IQR et Z-scores
            4. **Classification automatique** des types de variables
            5. **Validation rigoureuse** des hypoth√®ses
            """)

    # ============================================================
    # TAB 2: PR√âTRAITEMENT
    # Options pour nettoyer et pr√©parer les donn√©es pour l'analyse.
    # ============================================================
    with tab2:
        st.subheader("üõ†Ô∏è Options de pr√©traitement")

        st.markdown("""
        - Traitement intelligent des valeurs manquantes
        - D√©tection et correction des anomalies
        - Conservation rigoureuse des types de donn√©es
        - Normalisation adaptative selon la distribution
        """)

        # Options de pr√©traitement
        col_opt1, col_opt2 = st.columns(2)

        with col_opt1:
            strategy = st.selectbox(
                "Strat√©gie de pr√©traitement:",
                ["conservative", "balanced", "aggressive"],
                help="Conservative: privil√©gie la conservation des donn√©es\n"
                     "Balanced: √©quilibre entre conservation et traitement\n"
                     "Aggressive: optimisation maximale pour le machine learning"
            )

            target_column = st.selectbox(
                "Colonne cible (optionnelle):",
                ["Aucune"] + list(df.columns),
                index=0,
                help="S√©lectionnez une colonne cible pour un traitement adapt√©"
            )

        with col_opt2:
            preserve_types = st.checkbox("Conserver les types originaux", value=True)
            handle_missing = st.checkbox("Traiter les valeurs manquantes", value=True)
            handle_outliers = st.checkbox("Traiter les anomalies", value=True)

        if target_column == "Aucune":
            target_column = None

        if st.button("‚ö° Ex√©cuter le pr√©traitement", type="primary", use_container_width=True):
            with st.spinner("üîß Pr√©traitement en cours..."):
                try:
                    # Ex√©cuter le pr√©traitement scientifique
                    df_processed = processor.scientific_preprocess(
                        df,
                        target_column=target_column,
                        strategy=strategy
                    )

                    # Sauvegarder les r√©sultats
                    st.session_state.df_processed = df_processed
                    st.session_state.data_processor = processor

                    st.success(f"‚úÖ Pr√©traitement termin√©: {len(df)} ‚Üí {len(df_processed)} lignes")

                    # Afficher le rapport scientifique
                    scientific_report = processor.get_scientific_report()

                    # M√©triques de pr√©traitement
                    st.markdown("#### üìä M√©triques de pr√©traitement")

                    quality_metrics = scientific_report.get("quality_metrics", {})
                    col_met1, col_met2, col_met3 = st.columns(3)

                    with col_met1:
                        completeness = quality_metrics.get("completeness", 0) * 100
                        st.metric("Compl√©tude", f"{completeness:.1f}%")

                    with col_met2:
                        type_rate = quality_metrics.get("type_conservation_rate", 0) * 100
                        st.metric("Types conserv√©s", f"{type_rate:.1f}%")

                    with col_met3:
                        if "numeric_variability" in quality_metrics:
                            variability = quality_metrics["numeric_variability"]
                            st.metric("Variabilit√©", f"{variability:.3f}")

                    # √âtapes appliqu√©es
                    steps_applied = scientific_report.get("steps_applied", [])
                    if steps_applied:
                        st.markdown("#### üìù √âtapes appliqu√©es")

                        for i, step in enumerate(steps_applied[:10], 1):  # Limiter √† 10 √©tapes
                            st.markdown(f"{i}. {step}")

                        if len(steps_applied) > 10:
                            st.info(f"... et {len(steps_applied) - 10} autres √©tapes")

                    # Aper√ßu des donn√©es trait√©es
                    with st.expander("üëÅÔ∏è Aper√ßu des donn√©es trait√©es", expanded=False):
                        st.dataframe(df_processed.head(10), use_container_width=True)

                        # Comparaison avant/apr√®s
                        col_comp1, col_comp2 = st.columns(2)

                        with col_comp1:
                            st.markdown("**Avant traitement:**")
                            st.metric("Lignes", len(df))
                            st.metric("Valeurs manquantes", df.isna().sum().sum())

                        with col_comp2:
                            st.markdown("**Apr√®s traitement:**")
                            st.metric("Lignes", len(df_processed))
                            st.metric("Valeurs manquantes", df_processed.isna().sum().sum())

                    # T√©l√©chargement des donn√©es trait√©es
                    st.markdown("---")
                    st.markdown("#### üì§ Export des donn√©es trait√©es")

                    csv = df_processed.to_csv(index=False, encoding='utf-8-sig')
                    st.download_button(
                        label="üíæ T√©l√©charger CSV",
                        data=csv,
                        file_name="donnees_traitees_scientifiques.csv",
                        mime="text/csv",
                        use_container_width=True
                    )

                except Exception as e:
                    st.error(f"‚ùå Erreur lors du pr√©traitement: {str(e)}")
                    st.code(traceback.format_exc())

    # ============================================================
    # TAB 3: D√âTECTION DE CIBLES
    # Identifie les variables potentielles √† utiliser comme cible pour la mod√©lisation
    # ============================================================
    with tab3:
        st.subheader("üéØ D√©tection de Variables Cibles")

        st.markdown("""
        **üîé M√©thodologie de d√©tection:**

        1. **Analyse s√©mantique** des noms de colonnes
        2. **Crit√®res statistiques** (cardinalit√©, distribution)
        3. **Tests d'√©quilibre** pour les variables cat√©gorielles
        4. **√âlimination** des variables trop d√©s√©quilibr√©es
        """)

        if st.button("üéØ D√©tecter les variables cibles", type="primary", use_container_width=True):
            with st.spinner("üîé D√©tection en cours..."):
                try:
                    # D√©tection des variables cibles potentielles
                    potential_targets = processor.detect_potential_targets(df)

                    if potential_targets:
                        st.success(f"‚úÖ {len(potential_targets)} variables cibles potentielles d√©tect√©es")

                        # Afficher les variables cibles
                        st.markdown("#### üìÑ Variables cibles potentielles")

                        targets_df = pd.DataFrame({
                            "Variable": potential_targets,
                            "Type": [str(df[col].dtype) for col in potential_targets],
                            "Valeurs uniques": [df[col].nunique() for col in potential_targets],
                            "Valeurs manquantes": [df[col].isna().sum() for col in potential_targets]
                        })

                        st.dataframe(targets_df, use_container_width=True)

                        # D√©tails par variable cible
                        st.markdown("#### üìä Analyse d√©taill√©e des variables cibles")

                        for target_col in potential_targets[:5]:  # Limiter √† 5 variables
                            with st.expander(f"Analyse de: {target_col}", expanded=False):
                                col_target1, col_target2 = st.columns(2)

                                with col_target1:
                                    # Statistiques de base
                                    st.markdown("**Statistiques:**")
                                    non_null = df[target_col].dropna()

                                    if pd.api.types.is_numeric_dtype(df[target_col]):
                                        st.markdown(f"‚Ä¢ Moyenne: {non_null.mean():.2f}")
                                        st.markdown(f"‚Ä¢ √âcart-type: {non_null.std():.2f}")
                                        st.markdown(f"‚Ä¢ Min: {non_null.min():.2f}")
                                        st.markdown(f"‚Ä¢ Max: {non_null.max():.2f}")
                                    else:
                                        value_counts = df[target_col].value_counts(normalize=True)
                                        for val, prop in list(value_counts.items())[:5]:
                                            st.markdown(f"‚Ä¢ {val}: {prop * 100:.1f}%")

                                with col_target2:
                                    # Visualisation
                                    st.markdown("**Distribution:**")

                                    if pd.api.types.is_numeric_dtype(df[target_col]):
                                        try:
                                            import plotly.express as px

                                            fig = px.histogram(df, x=target_col, title=f"Distribution de {target_col}")
                                            st.plotly_chart(fig, use_container_width=True, height=300)
                                        except:
                                            st.info("üìä Visualisation non disponible")
                                    elif df[target_col].nunique() <= 10:
                                        try:
                                            import plotly.express as px

                                            value_counts = df[target_col].value_counts()
                                            fig = px.pie(values=value_counts.values,
                                                         names=value_counts.index,
                                                         title=f"Distribution de {target_col}")
                                            st.plotly_chart(fig, use_container_width=True, height=300)
                                        except:
                                            st.info("üìä Visualisation non disponible")

                        if len(potential_targets) > 5:
                            st.info(f"... et {len(potential_targets) - 5} autres variables cibles")

                        # Recommandations
                        st.markdown("#### üí° Recommandations")

                        if len(potential_targets) >= 3:
                            st.success("‚úÖ Plusieurs variables cibles potentielles d√©tect√©es.")
                            st.info("Pour la mod√©lisation, choisissez une variable avec:")
                            st.markdown("1. **Distribution √©quilibr√©e** (pas trop d√©s√©quilibr√©e)")
                            st.markdown("2. **Peu de valeurs manquantes**")
                            st.markdown("3. **Sens m√©tier clair**")
                        else:
                            st.warning("‚ö†Ô∏è Peu de variables cibles d√©tect√©es.")
                            st.info("Consid√©rez:")
                            st.markdown("1. **Cr√©er une variable cible d√©riv√©e**")
                            st.markdown("2. **Utiliser une colonne num√©rique comme cible**")
                            st.markdown("3. **Recoder une variable existante**")

                    else:
                        st.warning("‚ö†Ô∏è Aucune variable cible potentielle d√©tect√©e")
                        st.info("""
                        **Suggestions:**
                        1. V√©rifiez si vos donn√©es contiennent des colonnes comme:
                           - `risque`, `churn`, `resilie`, `renouvelle`
                           - Variables binaires (oui/non, 0/1)
                           - Variables cat√©gorielles √† faible cardinalit√©

                        2. Vous pouvez cr√©er une variable cible manuellement
                        3. Utilisez une variable num√©rique comme cible de r√©gression
                        """)

                except Exception as e:
                    st.error(f"‚ùå Erreur lors de la d√©tection: {str(e)}")
                    st.code(traceback.format_exc())

    # ============================================================
    # TAB 4: STATISTIQUES
    # Affiche des statistiques d√©taill√©es sur les colonnes et permet leur exploration
    # ============================================================
    with tab4:
        st.subheader("üìä Statistiques Compl√®tes")

        # Options d'affichage
        col_stats1, col_stats2 = st.columns(2)

        with col_stats1:
            show_detailed = st.checkbox("Afficher les statistiques d√©taill√©es", value=False)
            include_missing = st.checkbox("Inclure l'analyse des valeurs manquantes", value=True)

        with col_stats2:
            limit_cols = st.slider("Nombre maximum de colonnes √† afficher",
                                   min_value=5, max_value=50, value=20)
            sort_by = st.selectbox("Trier par:",
                                   ["Nom", "Type", "Valeurs manquantes", "Valeurs uniques"])

        if st.button("üìà G√©n√©rer les statistiques", type="primary", use_container_width=True):
            with st.spinner("üìä Calcul des statistiques..."):
                try:
                    # Obtenir les statistiques des colonnes
                    column_stats = processor.get_column_statistics(df)

                    # Convertir en DataFrame pour l'affichage
                    stats_list = []
                    for col, stats in column_stats.items():
                        stats_list.append({
                            "Colonne": col,
                            "Type": stats["dtype"],
                            "Non nul": stats["non_null_count"],
                            "Nul": stats["null_count"],
                            "% Nul": stats["null_percentage"],
                            "Uniques": stats["unique_count"],
                            "% Unique": (stats["unique_count"] / len(df)) * 100 if len(df) > 0 else 0
                        })

                    stats_df = pd.DataFrame(stats_list)

                    # Trier selon le crit√®re s√©lectionn√©
                    sort_map = {
                        "Nom": "Colonne",
                        "Type": "Type",
                        "Valeurs manquantes": "% Nul",
                        "Valeurs uniques": "Uniques"
                    }

                    if sort_by in sort_map:
                        stats_df = stats_df.sort_values(sort_map[sort_by],
                                                        ascending=(sort_by == "Nom"))

                    # Limiter le nombre de colonnes affich√©es
                    stats_df = stats_df.head(limit_cols)

                    st.success(f"‚úÖ Statistiques g√©n√©r√©es pour {len(column_stats)} colonnes")

                    # Tableau r√©capitulatif
                    st.markdown("#### üìÑ R√©capitulatif des colonnes")
                    st.dataframe(stats_df, use_container_width=True)

                    # Statistiques globales
                    st.markdown("#### üéØ Statistiques globales")

                    col_glob1, col_glob2, col_glob3, col_glob4 = st.columns(4)

                    with col_glob1:
                        total_missing = stats_df["Nul"].sum()
                        total_values = len(df) * len(df.columns)
                        missing_pct = (total_missing / total_values * 100) if total_values > 0 else 0
                        st.metric("Valeurs manquantes", f"{missing_pct:.1f}%")

                    with col_glob2:
                        avg_unique = stats_df["% Unique"].mean()
                        st.metric("Unicit√© moyenne", f"{avg_unique:.1f}%")

                    with col_glob3:
                        numeric_cols = df.select_dtypes(include=[np.number]).columns
                        st.metric("Colonnes num√©riques", len(numeric_cols))

                    with col_glob4:
                        categorical_cols = df.select_dtypes(include=['object']).columns
                        st.metric("Colonnes cat√©gorielles", len(categorical_cols))

                    # Analyse d√©taill√©e pour les colonnes s√©lectionn√©es
                    if show_detailed and len(stats_df) > 0:
                        st.markdown("#### üîç Analyse d√©taill√©e par colonne")

                        selected_cols = st.multiselect(
                            "S√©lectionnez les colonnes √† analyser en d√©tail:",
                            stats_df["Colonne"].tolist(),
                            default=stats_df["Colonne"].head(3).tolist()
                        )

                        for col in selected_cols:
                            if col in column_stats:
                                col_info = column_stats[col]

                                with st.expander(f"Analyse d√©taill√©e: {col}", expanded=False):
                                    # Informations de base
                                    col_det1, col_det2 = st.columns(2)

                                    with col_det1:
                                        st.markdown("**Informations de base:**")
                                        st.markdown(f"- Type: {col_info['dtype']}")
                                        st.markdown(f"- Non nul: {col_info['non_null_count']}")
                                        st.markdown(
                                            f"- Nul: {col_info['null_count']} ({col_info['null_percentage']:.1f}%)")
                                        st.markdown(f"- Uniques: {col_info['unique_count']}")

                                    with col_det2:
                                        if 'mean' in col_info:
                                            st.markdown("**Statistiques num√©riques:**")
                                            st.markdown(f"- Moyenne: {col_info['mean']:.2f}")
                                            st.markdown(f"- √âcart-type: {col_info['std']:.2f}")
                                            st.markdown(f"- Min: {col_info['min']:.2f}")
                                            st.markdown(f"- Max: {col_info['max']:.2f}")
                                            st.markdown(f"- M√©diane: {col_info.get('median', 'N/A')}")

                                    # √âchantillon de valeurs
                                    if col_info['sample_values']:
                                        st.markdown("**√âchantillon de valeurs:**")
                                        sample_str = ", ".join(str(v) for v in col_info['sample_values'][:10])
                                        st.code(sample_str)

                                    # Visualisation
                                    st.markdown("**Visualisation:**")

                                    if pd.api.types.is_numeric_dtype(df[col]):
                                        try:
                                            import plotly.express as px

                                            tab_viz1, tab_viz2 = st.tabs(["Histogramme", "Bo√Æte √† moustaches"])

                                            with tab_viz1:
                                                fig = px.histogram(df, x=col, title=f"Distribution de {col}")
                                                st.plotly_chart(fig, use_container_width=True, height=300)

                                            with tab_viz2:
                                                fig = px.box(df, y=col, title=f"Bo√Æte √† moustaches de {col}")
                                                st.plotly_chart(fig, use_container_width=True, height=300)

                                        except:
                                            st.info("üìä Visualisation non disponible")

                                    elif df[col].nunique() <= 20:
                                        try:
                                            import plotly.express as px

                                            value_counts = df[col].value_counts().head(10)
                                            fig = px.bar(x=value_counts.index, y=value_counts.values,
                                                         title=f"Fr√©quence des cat√©gories - {col}")
                                            fig.update_layout(xaxis_title=col, yaxis_title="Fr√©quence")
                                            st.plotly_chart(fig, use_container_width=True, height=300)
                                        except:
                                            st.info("üìä Visualisation non disponible")

                    # Export des statistiques
                    st.markdown("---")
                    st.markdown("#### üì§ Export des statistiques")

                    # Convertir en JSON pour l'export
                    import json

                    stats_json = json.dumps(column_stats, indent=2, ensure_ascii=False)

                    col_exp1, col_exp2 = st.columns(2)

                    with col_exp1:
                        st.download_button(
                            label="üíæ T√©l√©charger JSON",
                            data=stats_json,
                            file_name="statistiques_colonnes.json",
                            mime="application/json",
                            use_container_width=True
                        )

                    with col_exp2:
                        # Export CSV du r√©capitulatif
                        csv_stats = stats_df.to_csv(index=False, encoding='utf-8-sig')
                        st.download_button(
                            label="üíæ T√©l√©charger CSV",
                            data=csv_stats,
                            file_name="recapitulatif_statistiques.csv",
                            mime="text/csv",
                            use_container_width=True
                        )

                except Exception as e:
                    st.error(f"‚ùå Erreur lors du calcul des statistiques: {str(e)}")
                    st.code(traceback.format_exc())

# ============================================================
# üîí NLQ S√âCURIS√â - VERSION FINALE OPTIMIS√âE
# Interface de questions-r√©ponses en langage naturel sur les donn√©es.
# Version optimis√©e avec gestion de la s√©curit√© des donn√©es
# ============================================================
elif page == " üí¨ Assistant IA":
    st.header("üîí Assistant IA ")

    # V√©rification des donn√©es
    if not st.session_state.data_loaded:
        st.warning("‚ö†Ô∏è Veuillez d'abord charger des donn√©es dans l'onglet 'Chargement des donn√©es'")
        st.stop()

    # ============================================
    # INITIALISATION DU MOTEUR NLQ
    # ============================================
    if st.session_state.nlq_engine is None:
        try:
            from modules.secure_nlq_engine import SecureNLQEngine

            # R√©cup√©ration de la cl√© API
            api_key = None
            if hasattr(st.session_state, 'openai_client') and st.session_state.openai_client:
                api_key = st.session_state.openai_client.api_key
            elif 'api_key_input' in st.session_state and st.session_state.api_key_input:
                api_key = st.session_state.api_key_input

            if not api_key:
                st.error("‚ùå Aucune cl√© API OpenAI disponible")
                st.info("üí° Veuillez entrer une cl√© API valide dans la barre lat√©rale")
                st.stop()

            # Initialisation du moteur
            st.session_state.nlq_engine = SecureNLQEngine(api_key=api_key)
            st.success("‚úÖ Obtenez des r√©ponses en toutes simplicit√©")

        except ImportError as e:
            st.error(f"‚ùå Erreur : {e}")
            st.info("""
            **üìã V√©rifications n√©cessaires:**
            1. Le fichier `secure_nlq_engine.py` doit √™tre dans `modules/`
            2. Installez: `pip install openai pandas plotly`
            """)
            st.stop()
        except Exception as e:
            st.error(f"‚ùå Erreur d'initialisation NLQ: {e}")
            st.info("üîë V√©rifiez votre cl√© API et votre connexion Internet")
            st.stop()

    # V√©rification finale
    if st.session_state.nlq_engine is None:
        st.error("‚ùå Erreur dans le moteur de recherche")
        st.stop()

    # R√©cup√©ration des objets
    df = st.session_state.dataframe
    nlq_engine = st.session_state.nlq_engine

    # Initialisation de l'historique
    if 'nlq_history' not in st.session_state:
        st.session_state.nlq_history = []

    # ============================================
    # ONGLETS PRINCIPAUX
    # ============================================
    tab1, tab2, tab3, tab4 = st.tabs([
        "üìù Analyse par requ√™te",
        "üìä M√©tadonn√©es",
        "üîÑ Historique",
        "‚öôÔ∏è Configuration"
    ])

    # ============================================
    # TAB 1: ANALYSE PAR REQU√äTE
    # Interface principale pour poser des questions en langage naturel.
    # ============================================
    with tab1:
        st.subheader("üîç Analyse NLQ S√©curis√©e")

        # Information importante
        st.info("""
        **üéØ ANALYSE INTELLIGENTE PAR IA**

        Posez des questions en langage naturel sur vos donn√©es et le contexte
        """)

        # Zone de requ√™te
        col_query, col_tips = st.columns([3, 1])

        with col_query:
            st.markdown("### üí¨ Posez vos questions ")

            user_query = st.text_area(
                "",
                height=150,
                placeholder="""
Quelle est la distribution des primes d'assurance par type de v√©hicule? """,
                help="Formulez votre question en fran√ßais naturel. Soyez aussi pr√©cis que possible.",
                key="nlq_query_textarea"
            )

        with col_tips:
            st.markdown("**üìã Guide de formulation**")
            st.markdown("""
            **‚úÖ Bonnes pratiques:**
            - Soyez sp√©cifique
            - Mentionnez les variables cl√©s
            - D√©finissez l'objectif
            - Pr√©cisez le contexte qui vous int√©resse

            **‚ùå √Ä √©viter:**
            - Questions trop vagues
            - Termes ambigus
            - Multiples questions en une
            """)

        # Options d'analyse
        with st.expander("‚öôÔ∏è Options d'analyse avanc√©es", expanded=False):
            col_opt1, col_opt2 = st.columns(2)

            with col_opt1:
                use_data = st.checkbox(
                    "Utiliser les donn√©es r√©elles (g√©n√®re des graphiques)",
                    value=True,
                    help="Active la g√©n√©ration de graphiques bas√©s sur vos donn√©es"
                )

                max_samples = st.slider(
                    "√âchantillons pour l'analyse",
                    min_value=100,
                    max_value=5000,
                    value=1000,
                    step=100,
                    help="Nombre d'√©chantillons √† utiliser pour optimiser la performance"
                )

            with col_opt2:
                analysis_depth = st.select_slider(
                    "Profondeur d'analyse",
                    options=["Rapide", "Standard", "Approfondi"],
                    value="Standard"
                )

                include_viz = st.checkbox(
                    "Inclure les suggestions de visualisation",
                    value=True
                )

        # Bouton d'analyse
        st.markdown("---")

        if st.button("üöÄ Demander", type="primary", use_container_width=True):
            if user_query and user_query.strip():
                with st.spinner("üß† Analyse est en cours... Cela peut prendre juste quelques secondes."):
                    try:
                        # Choix du mode d'analyse
                        if use_data:
                            # Analyse avec donn√©es r√©elles
                            result = nlq_engine.analyze_query_with_data(
                                user_query=user_query,
                                dataframe=df,
                                max_samples=max_samples
                            )
                        else:
                            # Analyse avec m√©tadonn√©es uniquement
                            if not st.session_state.metadata:
                                # G√©n√©rer m√©tadonn√©es basiques
                                from modules.metadata_extractor import MetadataExtractor

                                metadata_extractor = MetadataExtractor(df)
                                st.session_state.metadata = metadata_extractor.extract_safe_metadata()

                            result = nlq_engine.analyze_query_with_metadata(
                                user_query=user_query,
                                metadata=st.session_state.metadata
                            )

                        # Sauvegarder dans l'historique
                        st.session_state.nlq_history.append({
                            "query": user_query,
                            "timestamp": datetime.now().isoformat(),
                            "result": result,
                            "mode": "with_data" if use_data else "metadata_only"
                        })

                        # Afficher les r√©sultats
                        if result.get("status") == "error":
                            st.error(f"‚ùå Erreur: {result.get('error', 'Erreur inconnue')}")
                        else:
                            st.success("‚úÖ Analyse termin√©e avec succ√®s!")

                            analysis = result.get("analysis", {})

                            # Intention et synth√®se
                            st.markdown("### üéØ Synth√®se de l'analyse")
                            intention = analysis.get("intention", "Analyse g√©n√©r√©e")
                            st.info(f"**Objectif identifi√©:** {intention}")

                            # Strat√©gie d'analyse
                            st.markdown("### üìã M√©thodologie")
                            strategie = analysis.get("strategie_analyse", "")
                            if strategie:
                                st.markdown(strategie)

                            # R√©ponse d√©taill√©e
                            st.markdown("### üìù Analyse compl√®te")
                            reponse = analysis.get("reponse_detaillee", "")
                            if reponse:
                                st.markdown(reponse)

                            # Insights cl√©s
                            insights = analysis.get("insights_cles", [])
                            if insights:
                                st.markdown("### üí° Insights cl√©s")
                                for i, insight in enumerate(insights, 1):
                                    st.markdown(f"{i}. {insight}")

                            # Recommandations
                            recommandations = analysis.get("recommandations", [])
                            if recommandations:
                                st.markdown("### üéØ Recommandations")
                                for i, reco in enumerate(recommandations, 1):
                                    st.markdown(f"{i}. {reco}")

                            # Graphiques g√©n√©r√©s
                            graphs = result.get("graphs", {})
                            if graphs and graphs.get("generated"):
                                st.markdown("### üìä Visualisations g√©n√©r√©es")

                                for graph in graphs["generated"]:
                                    st.markdown(f"#### {graph.get('description', graph['type'])}")
                                    st.markdown(f"*Variables: {', '.join(graph['variables'])}*")

                                    # Afficher le graphique HTML
                                    import streamlit.components.v1 as components

                                    components.html(graph["html"], height=500, scrolling=True)

                            # Informations sur l'√©chantillon
                            if "sample_info" in result:
                                with st.expander("‚ÑπÔ∏è Informations sur l'analyse", expanded=False):
                                    sample_info = result["sample_info"]
                                    col_info1, col_info2, col_info3 = st.columns(3)

                                    with col_info1:
                                        st.metric("Lignes analys√©es", f"{sample_info.get('sampled_rows', 0):,}")
                                    with col_info2:
                                        st.metric("Lignes totales", f"{sample_info.get('original_rows', 0):,}")
                                    with col_info3:
                                        st.metric("Colonnes", sample_info.get('columns', 0))

                    except Exception as e:
                        st.error(f"‚ùå Erreur lors de l'analyse: {str(e)}")
                        with st.expander("üîç D√©tails de l'erreur"):
                            st.code(traceback.format_exc())
            else:
                st.warning("‚ö†Ô∏è Veuillez entrer une question")

    # ============================================
    # TAB 2: M√âTADONN√âES
    # # Affiche les m√©tadonn√©es disponibles pour l'analyse NLQ.
    # ============================================
    with tab2:
        st.subheader("üìä M√©tadonn√©es disponibles pour l'analyse")

        if st.session_state.metadata:
            metadata = st.session_state.metadata

            # Vue d'ensemble
            col_meta1, col_meta2, col_meta3, col_meta4 = st.columns(4)

            general_info = metadata.get('general_info', {})
            with col_meta1:
                st.metric("Colonnes", general_info.get('nombre_colonnes', 0))
            with col_meta2:
                st.metric("Lignes", f"{general_info.get('nombre_lignes', 0):,}")
            with col_meta3:
                domaine = metadata.get('business_context_hints', {}).get('domaine', 'N/A')
                st.metric("Domaine", domaine.title())
            with col_meta4:
                completude = 100 - general_info.get('pourcentage_manquants_global', 0)
                st.metric("Compl√©tude", f"{completude:.1f}%")

            # Variables cl√©s
            st.markdown("#### üîë Variables cl√©s identifi√©es")
            key_vars = metadata.get('business_context_hints', {}).get('variables_cles', [])
            if key_vars:
                cols_key = st.columns(min(len(key_vars), 5))
                for i, var in enumerate(key_vars[:5]):
                    with cols_key[i]:
                        st.info(f"**{var}**")
            else:
                st.info("Aucune variable cl√© identifi√©e automatiquement")

            # Structure des colonnes
            st.markdown("#### üèóÔ∏è Structure des colonnes")

            columns_info = metadata.get('structure_columns', [])
            if columns_info:
                # Cr√©er un DataFrame pour affichage
                display_data = []
                for col_info in columns_info[:20]:  # Limiter √† 20
                    display_data.append({
                        "Colonne": col_info['nom'],
                        "Type": col_info['type_donnee'],
                        "Valeurs uniques": col_info.get('valeurs_uniques', 'N/A'),
                        "% Manquants": f"{col_info.get('pourcentage_manquants', 0):.1f}%"
                    })

                st.dataframe(pd.DataFrame(display_data), use_container_width=True)

                if len(columns_info) > 20:
                    st.info(f"... et {len(columns_info) - 20} autres colonnes")
        else:
            st.warning("‚ö†Ô∏è Aucune m√©tadonn√©e disponible")
            st.info("G√©n√©rez les m√©tadonn√©es dans l'onglet 'M√©tadonn√©es' pour activer cette fonctionnalit√©")

    # ============================================
    # TAB 3: HISTORIQUE
    # Affiche l'historique des analyses NLQ effectu√©es.
    # ============================================
    with tab3:
        st.subheader("üîÑ Historique des analyses NLQ")

        if st.session_state.nlq_history:
            st.info(f"üìã {len(st.session_state.nlq_history)} analyse(s) effectu√©e(s)")

            # Bouton pour effacer l'historique
            if st.button("üóëÔ∏è Effacer l'historique", type="secondary"):
                st.session_state.nlq_history = []
                st.success("‚úÖ Historique effac√©")
                st.rerun()

            st.markdown("---")

            # Afficher les analyses (les plus r√©centes en premier)
            for i, item in enumerate(reversed(st.session_state.nlq_history)):
                timestamp = item.get('timestamp', 'Date inconnue')
                query = item['query']
                mode = item.get('mode', 'unknown')

                # Ic√¥ne selon le mode
                mode_icon = "üìä" if mode == "with_data" else "üìã"

                with st.expander(f"{mode_icon} {timestamp[:19]} - {query[:60]}...",
                                 expanded=(i == 0)):

                    st.markdown(f"**Question compl√®te:** {query}")
                    st.caption(f"Mode: {'Avec donn√©es r√©elles' if mode == 'with_data' else 'M√©tadonn√©es uniquement'}")

                    result = item.get('result', {})

                    if result.get('status') == 'error':
                        st.error(f"‚ùå Erreur: {result.get('error', 'Erreur inconnue')}")
                    elif 'analysis' in result:
                        analysis = result['analysis']

                        # Intention
                        if 'intention' in analysis:
                            st.markdown(f"**üéØ Intention:** {analysis['intention']}")

                        # Strat√©gie
                        if 'strategie_analyse' in analysis:
                            st.markdown(f"**üìã M√©thodologie:** {analysis['strategie_analyse'][:200]}...")

                        # Nombre de graphiques
                        graphs = result.get('graphs', {})
                        nb_graphs = len(graphs.get('generated', []))
                        if nb_graphs > 0:
                            st.success(f"‚úÖ {nb_graphs} graphique(s) g√©n√©r√©(s)")

                        # Bouton pour r√©g√©n√©rer
                        if st.button(f"üîÑ R√©g√©n√©rer cette analyse",
                                     key=f"regen_{i}",
                                     use_container_width=True):
                            st.session_state.nlq_quick_query = query
                            st.rerun()
        else:
            st.info("üì≠ Aucune analyse dans l'historique")
            st.markdown("Lancez votre premi√®re analyse dans l'onglet **'Analyse par requ√™te'**")

    # ============================================
    # TAB 4: CONFIGURATION
    # Affiche la configuration du moteur NLQ et des options syst√®mes
    # ============================================
    with tab4:
        st.subheader("‚öôÔ∏è Configuration du moteur NLQ")

        # Statut
        st.markdown("#### üîß Statut du syst√®me")

        col_status1, col_status2 = st.columns(2)

        with col_status1:
            if st.session_state.nlq_engine:
                st.success("‚úÖ Moteur NLQ: Op√©rationnel")
                st.info(f"ü§ñ Mod√®le: {st.session_state.nlq_engine.model}")
            else:
                st.error("‚ùå Moteur NLQ: Non initialis√©")

        with col_status2:
            if st.session_state.openai_client:
                st.success("‚úÖ Client OpenAI: Connect√©")
            else:
                st.warning("‚ö†Ô∏è Client OpenAI: Non connect√©")

        # S√©curit√©
        st.markdown("#### üîí Niveau de s√©curit√©")
        st.success("""
        **Mode s√©curis√© activ√©:**
        - ‚úÖ Analyse sur m√©tadonn√©es uniquement (mode par d√©faut)
        - ‚úÖ Aucune donn√©e brute transmise √† l'API
        - ‚úÖ Anonymisation des requ√™tes
        - ‚úÖ Donn√©es en local uniquement
        - ‚úÖ Conformit√© RGPD garantie
        """)

        # Statistiques d'utilisation
        st.markdown("#### üìä Statistiques d'utilisation")

        col_stats1, col_stats2, col_stats3 = st.columns(3)

        with col_stats1:
            st.metric("Analyses totales", len(st.session_state.nlq_history))

        with col_stats2:
            analyses_reussies = sum(1 for item in st.session_state.nlq_history
                                    if item.get('result', {}).get('status') == 'success')
            st.metric("Analyses r√©ussies", analyses_reussies)

        with col_stats3:
            if st.session_state.nlq_history:
                dernier = st.session_state.nlq_history[-1]['timestamp']
                st.metric("Derni√®re analyse", dernier[:19])

        # Actions
        st.markdown("#### üõ†Ô∏è Actions")

        col_action1, col_action2 = st.columns(2)

        with col_action1:
            if st.button("üîÑ R√©initialiser le moteur NLQ", use_container_width=True):
                st.session_state.nlq_engine = None
                st.success("‚úÖ Moteur r√©initialis√©")
                st.info("Le moteur sera recharg√© √† la prochaine utilisation")

        with col_action2:
            if st.button("üì• Exporter l'historique (JSON)", use_container_width=True):
                if st.session_state.nlq_history:
                    import json

                    history_json = json.dumps(st.session_state.nlq_history,
                                              indent=2, ensure_ascii=False)
                    st.download_button(
                        label="üíæ T√©l√©charger l'historique",
                        data=history_json,
                        file_name=f"nlq_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                        mime="application/json"
                    )
                else:
                    st.warning("Aucun historique √† exporter")

        # Aide
        with st.expander("‚ùì Aide et documentation", expanded=False):
            st.markdown("""
            ### Guide d'utilisation du moteur NLQ

            **1. Modes d'analyse:**
            - **M√©tadonn√©es uniquement**: Analyse rapide et s√©curis√©e sur la structure des donn√©es
            - **Avec donn√©es r√©elles**: G√©n√®re des graphiques et analyses approfondies

            **2. Formulation des questions:**
            - Utilisez un langage naturel en fran√ßais
            - Soyez pr√©cis sur les variables d'int√©r√™t
            - Mentionnez l'objectif m√©tier

            **3. Optimisation:**
            - Limitez le nombre d'√©chantillons pour des analyses rapides
            - Utilisez les exemples rapides pour d√©marrer
            - Consultez l'historique pour retrouver vos analyses

            **4. S√©curit√©:**
            - Vos donn√©es ne quittent jamais votre environnement
            - Seules les m√©tadonn√©es sont utilis√©es (mode par d√©faut)
            - Conformit√© RGPD assur√©e
            """)
# ============================================================
# INSIGHTS AVANC√âS - VERSION COMPL√àTE AVEC TOUS LES GRAPHIQUES
# ============================================================
elif page == " üëÅÔ∏è Visualisation des donn√©es":
    st.header(" üëÅÔ∏è Visualisation des donn√©es et Analyse du Risque Client")

    # V√©rifier qu'on a des donn√©es charg√©es
    if not st.session_state.data_loaded:
        st.warning("‚ö†Ô∏è Veuillez d'abord charger des donn√©es dans l'onglet 'Chargement des donn√©es'.")
        st.stop()

    # Prendre les donn√©es pr√©par√©es si elles existent, sinon les donn√©es brutes
    if st.session_state.df_final is not None:
        df_final = st.session_state.df_final
    else:
        df_final = st.session_state.dataframe

    # V√©rifier que les colonnes essentielles existent
    required_columns = ['ncli', 'nomncli', 'Prime', 'nb_jour_couv']
    available_columns = df_final.columns.tolist()
    missing_columns = [col for col in required_columns if col not in available_columns]

    if missing_columns:
        st.error(f"‚ùå Colonnes requises manquantes: {missing_columns}")
        st.info(f"Colonnes disponibles: {', '.join(available_columns[:10])}...")
        st.stop()

    # Initialiser le moteur d'insights s'il n'existe pas encore
    if st.session_state.insight_engine is None:
        try:
            from modules.insight_engine import InsightEngine

            st.session_state.insight_engine = InsightEngine()
            st.success("‚úÖ Moteur d'insights initialis√©")
        except ImportError as e:
            st.error(f"‚ùå Impossible de charger insight_engine: {e}")
            st.info("Assurez-vous que insight_engine.py est dans le dossier modules/")
            st.stop()

    insight_engine = st.session_state.insight_engine

    # Construction de la table client et calcul des scores de risque
    with st.spinner("üîç Construction de la table client et calcul des risques..."):
        try:
            # Construire la table agr√©g√©e par client
            client_table = insight_engine.build_client_risk_table(df_final)

            # Calculer la m√©diane de la prime par jour pour r√©f√©rence
            ppj_median = client_table["prime_par_jour_moy"].median()

            # Calculer le score de risque pour chaque client
            scored_clients = insight_engine.compute_risk_score(client_table)

            # G√©n√©rer un insight personnalis√© pour chaque client
            scored_clients["insight"] = scored_clients.apply(
                lambda row: insight_engine.generate_client_insight(row, ppj_median),
                axis=1
            )

            # Sauvegarder tout √ßa dans la session
            st.session_state.scored_clients = scored_clients
            st.session_state.client_table = client_table
            st.session_state.raw_data = df_final

            st.success(f"‚úÖ Analyse des risques compl√©t√©e pour {len(scored_clients)} clients!")

        except Exception as e:
            st.error(f"‚ùå Erreur lors de l'analyse des risques: {str(e)}")
            st.code(traceback.format_exc())
            st.stop()

    st.markdown("---")

    # Menu pour choisir le type d'analyse
    analysis_type = st.radio(
        "**S√©lectionnez le type d'analyse :**",
        [
            "üìä Vue d'ensemble",
            "üìà Analyse Univari√©e",
            "üìâ Analyse Bivari√©e",
            "üéØ Analyse Multivari√©e",
            "üìÑ Rapport Narratif"
        ],
        horizontal=True
    )

    # ============================================================
    # VUE D'ENSEMBLE
    # ============================================================
    if analysis_type == "üìä Vue d'ensemble":
        st.subheader("üìä Vue d'ensemble du portefeuille")

        # Afficher les m√©triques principales
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Clients", f"{len(scored_clients):,}")
        with col2:
            st.metric("Prime totale", f"{scored_clients['prime_totale'].sum():,.0f} MAD")
        with col3:
            st.metric("Score risque moyen", f"{scored_clients['score_risque'].mean():.1f}/100")
        with col4:
            high_risk = (scored_clients['niveau_risque'] == '√âlev√©').sum()
            st.metric("Risque √©lev√©", f"{high_risk} clients")

        # G√©n√©rer et afficher les insights cl√©s
        st.subheader("üéØ Insights cl√©s")
        insights = insight_engine.generate_insights(scored_clients)
        for ins in insights:
            st.markdown(f"‚Ä¢ {ins}")

        # Graphiques de distribution
        col_viz1, col_viz2 = st.columns(2)

        with col_viz1:
            # Graphique en camembert de la r√©partition des risques
            st.subheader("üìä Distribution des risques")
            risk_dist = scored_clients["niveau_risque"].value_counts()
            fig_pie = go.Figure(data=[go.Pie(
                labels=risk_dist.index,
                values=risk_dist.values,
                hole=.3,
                marker_colors=['#2ECC71', '#F39C12', '#E74C3C'],
                textinfo='label+percent',
                textposition='inside'
            )])
            fig_pie.update_layout(
                title="R√©partition par niveau de risque",
                height=400
            )
            st.plotly_chart(fig_pie, use_container_width=True)

        with col_viz2:
            # Histogramme des scores de risque
            st.subheader("üìà Distribution des scores")
            fig_hist = px.histogram(
                scored_clients,
                x='score_risque',
                nbins=30,
                title="Distribution des scores de risque",
                color_discrete_sequence=['#3498DB']
            )
            fig_hist.update_layout(
                xaxis_title="Score de risque",
                yaxis_title="Nombre de clients",
                height=400
            )
            st.plotly_chart(fig_hist, use_container_width=True)

        # Graphique de r√©partition par tranches
        st.subheader("üìä R√©partition par tranches de risque")

        # Cr√©er des tranches de score
        scored_clients['tranche_score'] = pd.cut(
            scored_clients['score_risque'],
            bins=[0, 25, 50, 75, 100],
            labels=['0-25', '26-50', '51-75', '76-100']
        )

        tranche_counts = scored_clients['tranche_score'].value_counts().sort_index()
        fig_tranche = go.Figure(data=[go.Bar(
            x=tranche_counts.index.astype(str),
            y=tranche_counts.values,
            marker_color=['#2ECC71', '#F1C40F', '#E67E22', '#E74C3C'],
            text=tranche_counts.values,
            textposition='auto'
        )])
        fig_tranche.update_layout(
            title="Nombre de clients par tranche de score",
            xaxis_title="Tranche de score",
            yaxis_title="Nombre de clients",
            height=400
        )
        st.plotly_chart(fig_tranche, use_container_width=True)

        # Top 10 des clients √† risque √©lev√©
        st.subheader("üî¥ Top 10 clients √† risque √©lev√©")
        high_risk_clients = scored_clients[scored_clients['niveau_risque'] == '√âlev√©'].sort_values(
            'score_risque', ascending=False
        ).head(10)

        # Colonnes √† afficher
        display_columns = ['nomncli', 'score_risque', 'prime_totale', 'frequence_sinistre', 'insight']
        available_display = [col for col in display_columns if col in high_risk_clients.columns]

        if available_display:
            st.dataframe(
                high_risk_clients[available_display],
                use_container_width=True
            )
        else:
            st.info("‚ö†Ô∏è Aucune donn√©e disponible pour les clients √† haut risque")

    # ============================================================
    # ANALYSE UNIVARI√âE ENRICHIE
    # ============================================================
    elif analysis_type == "üìà Analyse Univari√©e":
        st.subheader("üìà Analyse Univari√©e Approfondie")

        col1, col2 = st.columns(2)

        with col1:
            # Lister les variables num√©riques disponibles
            numeric_cols = scored_clients.select_dtypes(include=[np.number]).columns.tolist()
            selected_var = st.selectbox(
                "S√©lectionnez une variable num√©rique :",
                options=numeric_cols,
                index=numeric_cols.index('score_risque') if 'score_risque' in numeric_cols else 0
            )

        with col2:
            chart_type = st.selectbox(
                "Type de visualisation :",
                ["Tous les graphiques", "Histogramme", "Bo√Æte √† moustaches", "Violin plot", "Statistiques descriptives"]
            )

        # Statistiques descriptives de la variable
        st.subheader(f"üìä Statistiques de : {selected_var}")

        col_stat1, col_stat2, col_stat3, col_stat4, col_stat5 = st.columns(5)
        with col_stat1:
            st.metric("Moyenne", f"{scored_clients[selected_var].mean():.2f}")
        with col_stat2:
            st.metric("M√©diane", f"{scored_clients[selected_var].median():.2f}")
        with col_stat3:
            st.metric("√âcart-type", f"{scored_clients[selected_var].std():.2f}")
        with col_stat4:
            st.metric("Min", f"{scored_clients[selected_var].min():.2f}")
        with col_stat5:
            st.metric("Max", f"{scored_clients[selected_var].max():.2f}")

        # Afficher les graphiques selon le type choisi
        if chart_type == "Tous les graphiques":
            col_g1, col_g2 = st.columns(2)

            with col_g1:
                # Histogramme
                fig_hist = px.histogram(
                    scored_clients,
                    x=selected_var,
                    nbins=30,
                    title=f"Histogramme - {selected_var}",
                    color_discrete_sequence=['#3498DB']
                )
                fig_hist.update_layout(height=350)
                st.plotly_chart(fig_hist, use_container_width=True)

                # Violin plot
                fig_violin = px.violin(
                    scored_clients,
                    y=selected_var,
                    box=True,
                    title=f"Violin Plot - {selected_var}",
                    color_discrete_sequence=['#9B59B6']
                )
                fig_violin.update_layout(height=350)
                st.plotly_chart(fig_violin, use_container_width=True)

            with col_g2:
                # Bo√Æte √† moustaches
                fig_box = px.box(
                    scored_clients,
                    y=selected_var,
                    title=f"Bo√Æte √† moustaches - {selected_var}",
                    color_discrete_sequence=['#E74C3C']
                )
                fig_box.update_layout(height=350)
                st.plotly_chart(fig_box, use_container_width=True)

                # Courbe de densit√©
                fig_density = go.Figure()
                fig_density.add_trace(go.Histogram(
                    x=scored_clients[selected_var],
                    histnorm='probability density',
                    name='Densit√©',
                    marker_color='#1ABC9C',
                    nbinsx=30
                ))
                fig_density.update_layout(
                    title=f"Courbe de densit√© - {selected_var}",
                    height=350
                )
                st.plotly_chart(fig_density, use_container_width=True)

        elif chart_type == "Histogramme":
            fig = px.histogram(
                scored_clients,
                x=selected_var,
                nbins=30,
                title=f"Histogramme - {selected_var}",
                color_discrete_sequence=['#3498DB']
            )
            st.plotly_chart(fig, use_container_width=True)

        elif chart_type == "Bo√Æte √† moustaches":
            fig = px.box(
                scored_clients,
                y=selected_var,
                title=f"Bo√Æte √† moustaches - {selected_var}",
                color_discrete_sequence=['#E74C3C']
            )
            st.plotly_chart(fig, use_container_width=True)

        elif chart_type == "Violin plot":
            fig = px.violin(
                scored_clients,
                y=selected_var,
                box=True,
                title=f"Violin Plot - {selected_var}",
                color_discrete_sequence=['#9B59B6']
            )
            st.plotly_chart(fig, use_container_width=True)

        else:  # Statistiques descriptives
            stats_df = scored_clients[selected_var].describe()
            st.dataframe(stats_df, use_container_width=True)

        # Analyse des variables cat√©gorielles
        st.markdown("---")
        st.subheader("üìä Analyse des variables cat√©gorielles")

        categorical_cols = scored_clients.select_dtypes(include=['object', 'category']).columns.tolist()

        if categorical_cols:
            cat_var = st.selectbox(
                "S√©lectionnez une variable cat√©gorielle :",
                options=categorical_cols,
                index=categorical_cols.index('niveau_risque') if 'niveau_risque' in categorical_cols else 0
            )

            if cat_var in scored_clients.columns:
                cat_dist = scored_clients[cat_var].value_counts()

                col_cat1, col_cat2 = st.columns(2)

                with col_cat1:
                    # Graphique en barres
                    fig_bar = go.Figure(data=[go.Bar(
                        x=cat_dist.index,
                        y=cat_dist.values,
                        marker_color='#3498DB',
                        text=cat_dist.values,
                        textposition='auto'
                    )])
                    fig_bar.update_layout(
                        title=f"Distribution de {cat_var}",
                        xaxis_title=cat_var,
                        yaxis_title="Nombre",
                        height=400
                    )
                    st.plotly_chart(fig_bar, use_container_width=True)

                with col_cat2:
                    # Graphique en secteurs
                    fig_pie_cat = px.pie(
                        values=cat_dist.values,
                        names=cat_dist.index,
                        title=f"R√©partition de {cat_var}"
                    )
                    fig_pie_cat.update_layout(height=400)
                    st.plotly_chart(fig_pie_cat, use_container_width=True)
        else:
            st.info("Aucune variable cat√©gorielle disponible")

    # ============================================================
    # ANALYSE BIVARI√âE ENRICHIE
    # ============================================================
    elif analysis_type == "üìâ Analyse Bivari√©e":
        st.subheader("üìâ Analyse Bivari√©e Approfondie")

        numeric_cols = scored_clients.select_dtypes(include=[np.number]).columns.tolist()

        col1, col2, col3 = st.columns(3)
        with col1:
            x_var = st.selectbox(
                "Variable X :",
                options=numeric_cols,
                index=numeric_cols.index('prime_par_jour_moy') if 'prime_par_jour_moy' in numeric_cols else 0
            )

        with col2:
            y_var = st.selectbox(
                "Variable Y :",
                options=numeric_cols,
                index=numeric_cols.index('score_risque') if 'score_risque' in numeric_cols else 1
            )

        with col3:
            color_var = st.selectbox(
                "Variable de couleur :",
                options=['Aucune'] + scored_clients.columns.tolist(),
                index=0
            )

        # Nuage de points principal
        st.subheader("üìä Nuage de points")
        fig_scatter = px.scatter(
            scored_clients,
            x=x_var,
            y=y_var,
            color=color_var if color_var != 'Aucune' else None,
            title=f"Relation entre {x_var} et {y_var}",
            trendline="ols",
            height=500
        )
        st.plotly_chart(fig_scatter, use_container_width=True)

        # Calculer et afficher la corr√©lation
        try:
            correlation = scored_clients[[x_var, y_var]].corr().iloc[0, 1]

            col_corr1, col_corr2, col_corr3 = st.columns(3)
            with col_corr1:
                st.metric("Coefficient de corr√©lation", f"{correlation:.3f}")

            with col_corr2:
                # Interpr√©ter la force de la corr√©lation
                if correlation > 0.7:
                    st.info("üìà Forte corr√©lation positive")
                elif correlation > 0.3:
                    st.info("‚ÜóÔ∏è Corr√©lation positive mod√©r√©e")
                elif correlation < -0.7:
                    st.info("üìâ Forte corr√©lation n√©gative")
                elif correlation < -0.3:
                    st.info("‚ÜòÔ∏è Corr√©lation n√©gative mod√©r√©e")
                else:
                    st.info("‚û°Ô∏è Faible ou pas de corr√©lation")

            with col_corr3:
                # Coefficient de d√©termination
                r2 = correlation ** 2
                st.metric("R¬≤ (variance expliqu√©e)", f"{r2:.3f}")

            # Interpr√©tation
            st.markdown("---")
            st.subheader("üí° Interpr√©tation")
            if correlation > 0:
                st.markdown(f"**Relation positive :** Quand {x_var} augmente, {y_var} tend √† augmenter")
            elif correlation < 0:
                st.markdown(f"**Relation n√©gative :** Quand {x_var} augmente, {y_var} tend √† diminuer")
            else:
                st.markdown("**Pas de relation lin√©aire √©vidente** entre les deux variables")

            # Graphiques suppl√©mentaires
            st.markdown("---")
            st.subheader("üìä Analyses compl√©mentaires")

            col_supp1, col_supp2 = st.columns(2)

            with col_supp1:
                # Heatmap de densit√©
                fig_density_2d = go.Figure(go.Histogram2d(
                    x=scored_clients[x_var],
                    y=scored_clients[y_var],
                    colorscale='Blues'
                ))
                fig_density_2d.update_layout(
                    title=f"Carte de densit√© - {x_var} vs {y_var}",
                    xaxis_title=x_var,
                    yaxis_title=y_var,
                    height=400
                )
                st.plotly_chart(fig_density_2d, use_container_width=True)

            with col_supp2:
                # Box plot group√© si une variable de couleur est s√©lectionn√©e
                if color_var != 'Aucune' and scored_clients[color_var].nunique() <= 10:
                    fig_box_grouped = px.box(
                        scored_clients,
                        x=color_var,
                        y=y_var,
                        title=f"{y_var} par {color_var}",
                        color=color_var
                    )
                    fig_box_grouped.update_layout(height=400)
                    st.plotly_chart(fig_box_grouped, use_container_width=True)
                else:
                    # Scatter plot avec taille de points
                    fig_bubble = px.scatter(
                        scored_clients,
                        x=x_var,
                        y=y_var,
                        size=abs(scored_clients[y_var]),
                        title=f"Bubble chart - {x_var} vs {y_var}",
                        opacity=0.6
                    )
                    fig_bubble.update_layout(height=400)
                    st.plotly_chart(fig_bubble, use_container_width=True)

        except Exception as e:
            st.warning(f"Impossible de calculer la corr√©lation: {str(e)}")

    # ============================================================
    # ANALYSE MULTIVARI√âE COMPL√àTE
    # ============================================================
    elif analysis_type == "üéØ Analyse Multivari√©e":
        st.subheader("üéØ Analyse Multivari√©e Compl√®te")

        # Menu pour choisir le type d'analyse multivari√©e
        multivariate_choice = st.selectbox(
            "Choisissez l'analyse multivari√©e :",
            [
                "Analyse en Composantes Principales (ACP)",
                "Analyse des Correspondances Multiples (ACM)",
                "Clustering (K-means)",
                "Matrice de corr√©lation"
            ]
        )

        if multivariate_choice == "Analyse en Composantes Principales (ACP)":
            st.subheader("üîÆ Analyse en Composantes Principales (ACP)")

            # R√©cup√©rer les variables disponibles pour l'ACP
            available_vars = insight_engine.get_available_variables_for_pca(scored_clients)

            if len(available_vars) < 2:
                st.warning(f"‚ö†Ô∏è Pas assez de variables num√©riques pour l'ACP. Variables disponibles: {available_vars}")
                st.stop()

            # Permettre √† l'utilisateur de s√©lectionner les variables
            selected_vars = st.multiselect(
                "S√©lectionnez les variables pour l'ACP :",
                options=available_vars,
                default=available_vars[:min(5, len(available_vars))]
            )

            if len(selected_vars) < 2:
                st.error("‚ùå Veuillez s√©lectionner au moins 2 variables pour l'ACP.")
                st.stop()

            # Param√®tres de l'ACP
            col_param1, col_param2 = st.columns(2)
            with col_param1:
                n_components = st.slider(
                    "Nombre de composantes principales :",
                    min_value=2,
                    max_value=min(10, len(selected_vars)),
                    value=min(3, len(selected_vars))
                )
            with col_param2:
                scale_data = st.checkbox("Standardiser les donn√©es", value=True)

            if st.button("üî¨ Lancer l'ACP", type="primary"):
                with st.spinner("Calcul de l'ACP en cours..."):
                    try:
                        from sklearn.decomposition import PCA
                        from sklearn.preprocessing import StandardScaler

                        # Pr√©parer les donn√©es
                        X_pca = scored_clients[selected_vars].dropna()

                        if scale_data:
                            scaler = StandardScaler()
                            X_scaled = scaler.fit_transform(X_pca)
                        else:
                            X_scaled = X_pca.values

                        # Appliquer l'ACP
                        pca = PCA(n_components=n_components)
                        components = pca.fit_transform(X_scaled)

                        st.success("‚úÖ ACP termin√©e avec succ√®s !")

                        # Variance expliqu√©e
                        st.subheader("üìä Variance expliqu√©e")

                        col_var1, col_var2 = st.columns(2)

                        with col_var1:
                            # Graphique de la variance expliqu√©e
                            fig_var = go.Figure()
                            fig_var.add_trace(go.Bar(
                                x=[f'CP{i + 1}' for i in range(n_components)],
                                y=pca.explained_variance_ratio_ * 100,
                                marker_color='#3498DB',
                                text=[f'{v:.1f}%' for v in pca.explained_variance_ratio_ * 100],
                                textposition='auto'
                            ))
                            fig_var.update_layout(
                                title="Variance expliqu√©e par composante",
                                xaxis_title="Composante",
                                yaxis_title="Variance expliqu√©e (%)",
                                height=400
                            )
                            st.plotly_chart(fig_var, use_container_width=True)

                        with col_var2:
                            # Variance cumul√©e
                            cumsum = np.cumsum(pca.explained_variance_ratio_ * 100)
                            fig_cumsum = go.Figure()
                            fig_cumsum.add_trace(go.Scatter(
                                x=[f'CP{i + 1}' for i in range(n_components)],
                                y=cumsum,
                                mode='lines+markers',
                                marker=dict(size=10, color='#E74C3C'),
                                line=dict(width=3)
                            ))
                            fig_cumsum.update_layout(
                                title="Variance cumul√©e",
                                xaxis_title="Composante",
                                yaxis_title="Variance cumul√©e (%)",
                                height=400
                            )
                            st.plotly_chart(fig_cumsum, use_container_width=True)

                        # Contributions des variables
                        st.subheader("üéØ Contributions des variables")

                        # Calculer les loadings
                        loadings = pca.components_.T * np.sqrt(pca.explained_variance_)
                        loadings_df = pd.DataFrame(
                            loadings,
                            columns=[f'CP{i + 1}' for i in range(n_components)],
                            index=selected_vars
                        )

                        # Heatmap des contributions
                        fig_loadings = px.imshow(
                            loadings_df.T,
                            title="Contributions des variables aux composantes",
                            labels=dict(x="Variable", y="Composante", color="Contribution"),
                            color_continuous_scale='RdBu',
                            aspect='auto'
                        )
                        fig_loadings.update_layout(height=400)
                        st.plotly_chart(fig_loadings, use_container_width=True)

                        # Tableau des contributions
                        st.dataframe(
                            loadings_df.style.background_gradient(cmap='RdBu', axis=0),
                            use_container_width=True
                        )

                        # Projection des individus
                        st.subheader("üìç Projection des individus")

                        # Cr√©er un dataframe avec les composantes
                        pca_df = pd.DataFrame(
                            components[:, :2],
                            columns=['CP1', 'CP2']
                        )

                        # Ajouter les informations des clients
                        pca_df['client'] = X_pca.index
                        if 'niveau_risque' in scored_clients.columns:
                            pca_df['niveau_risque'] = scored_clients.loc[X_pca.index, 'niveau_risque'].values

                        # Nuage de points des 2 premi√®res composantes
                        fig_proj = px.scatter(
                            pca_df,
                            x='CP1',
                            y='CP2',
                            color='niveau_risque' if 'niveau_risque' in pca_df.columns else None,
                            title="Projection sur les 2 premi√®res composantes principales",
                            hover_data=['client']
                        )
                        fig_proj.update_layout(height=500)
                        st.plotly_chart(fig_proj, use_container_width=True)

                        # Cercle de corr√©lation (biplot)
                        if n_components >= 2:
                            st.subheader("üîµ Cercle de corr√©lation")

                            fig_circle = go.Figure()

                            # Ajouter le cercle
                            theta = np.linspace(0, 2 * np.pi, 100)
                            fig_circle.add_trace(go.Scatter(
                                x=np.cos(theta),
                                y=np.sin(theta),
                                mode='lines',
                                line=dict(color='gray', dash='dash'),
                                showlegend=False
                            ))

                            # Ajouter les vecteurs des variables
                            for i, var in enumerate(selected_vars):
                                fig_circle.add_trace(go.Scatter(
                                    x=[0, loadings[i, 0]],
                                    y=[0, loadings[i, 1]],
                                    mode='lines+markers+text',
                                    name=var,
                                    text=['', var],
                                    textposition='top center',
                                    marker=dict(size=8)
                                ))

                            fig_circle.update_layout(
                                title="Cercle de corr√©lation (CP1 vs CP2)",
                                xaxis_title=f"CP1 ({pca.explained_variance_ratio_[0] * 100:.1f}%)",
                                yaxis_title=f"CP2 ({pca.explained_variance_ratio_[1] * 100:.1f}%)",
                                height=600,
                                showlegend=True
                            )
                            fig_circle.update_xaxes(range=[-1.1, 1.1], zeroline=True)
                            fig_circle.update_yaxes(range=[-1.1, 1.1], zeroline=True)
                            st.plotly_chart(fig_circle, use_container_width=True)

                    except Exception as e:
                        st.error(f"‚ùå Erreur lors de l'ACP: {str(e)}")
                        st.code(traceback.format_exc())
        elif multivariate_choice == "Analyse des Correspondances Multiples (ACM)":
            st.subheader("üé≠ Analyse des Correspondances Multiples (ACM)")

            # R√©cup√©rer les variables cat√©gorielles
            available_vars = insight_engine.get_available_variables_for_acm(scored_clients)

            if len(available_vars) < 2:
                st.warning(f"‚ö†Ô∏è Pas assez de variables cat√©gorielles pour l'ACM. Variables disponibles: {available_vars}")
                st.stop()

            # S√©lection des variables
            selected_vars = st.multiselect(
                "S√©lectionnez les variables cat√©gorielles pour l'ACM :",
                options=available_vars,
                default=available_vars[:min(5, len(available_vars))]
            )

            if len(selected_vars) < 2:
                st.error("‚ùå Veuillez s√©lectionner au moins 2 variables pour l'ACM.")
                st.stop()

            if st.button("üî¨ Lancer l'ACM", type="primary"):
                with st.spinner("Calcul de l'ACM en cours..."):
                    try:
                        # Ex√©cuter l'ACM
                        mca_result = insight_engine.perform_acm_analysis(
                            scored_clients,
                            selected_vars
                        )

                        if 'error' in mca_result:
                            st.error(f"‚ùå Erreur ACM: {mca_result['error']}")
                        else:
                            st.success("‚úÖ ACM termin√©e avec succ√®s !")

                            # Visualiser les r√©sultats
                            fig_mca = insight_engine.create_mca_visualization(mca_result, scored_clients)
                            if fig_mca:
                                st.plotly_chart(fig_mca, use_container_width=True)

                    except Exception as e:
                        st.error(f"‚ùå Erreur lors de l'ACM: {str(e)}")
                        st.code(traceback.format_exc())

        elif multivariate_choice == "Clustering (K-means)":
            st.subheader("üë• Clustering (K-means)")

            # Pr√©parer les variables pour le clustering
            numeric_cols = scored_clients.select_dtypes(include=[np.number]).columns.tolist()
            exclude_cols = ['ncli', 'score_risque']
            analysis_cols = [col for col in numeric_cols if col not in exclude_cols]

            if len(analysis_cols) >= 2:
                # Proposer des variables recommand√©es bas√©es sur vos colonnes
                recommended_vars = ['prime_par_jour_moy', 'duree_moyenne', 'nb_avenants',
                                    'frequence_sinistre', 'retard_paiement_moyen']
                available_recommended = [v for v in recommended_vars if v in analysis_cols]

                clustering_features = st.multiselect(
                    "S√©lectionnez les variables pour le clustering :",
                    options=analysis_cols,
                    default=available_recommended[
                        :min(3, len(available_recommended))] if available_recommended else analysis_cols[
                        :min(3, len(analysis_cols))]
                )

                if len(clustering_features) < 2:
                    st.error("‚ùå Veuillez s√©lectionner au moins 2 variables pour le clustering.")
                    st.stop()

                # Param√®tres du clustering
                col_clust1, col_clust2 = st.columns(2)
                with col_clust1:
                    n_clusters = st.slider("Nombre de clusters :", min_value=2, max_value=10, value=3)
                with col_clust2:
                    show_elbow = st.checkbox("Afficher la m√©thode du coude", value=True)

                if st.button("üî¨ Lancer le Clustering", type="primary"):
                    with st.spinner("Clustering en cours..."):
                        try:
                            from sklearn.cluster import KMeans
                            from sklearn.preprocessing import StandardScaler

                            # Pr√©parer les donn√©es
                            X_cluster = scored_clients[clustering_features].fillna(0)

                            # Normaliser les donn√©es
                            scaler = StandardScaler()
                            X_scaled = scaler.fit_transform(X_cluster)

                            # M√©thode du coude si demand√©e
                            if show_elbow:
                                st.subheader("üìâ M√©thode du coude")
                                inertias = []
                                K_range = range(2, min(11, len(X_cluster)))

                                for k in K_range:
                                    kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)
                                    kmeans_temp.fit(X_scaled)
                                    inertias.append(kmeans_temp.inertia_)

                                fig_elbow = go.Figure()
                                fig_elbow.add_trace(go.Scatter(
                                    x=list(K_range),
                                    y=inertias,
                                    mode='lines+markers',
                                    marker=dict(size=10, color='#E74C3C'),
                                    line=dict(width=3)
                                ))
                                fig_elbow.update_layout(
                                    title="M√©thode du coude - D√©termination du nombre optimal de clusters",
                                    xaxis_title="Nombre de clusters",
                                    yaxis_title="Inertie",
                                    height=400
                                )
                                st.plotly_chart(fig_elbow, use_container_width=True)

                            # Appliquer K-means
                            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
                            clusters = kmeans.fit_predict(X_scaled)

                            # Ajouter les clusters au dataframe
                            scored_clients_clustered = scored_clients.copy()
                            scored_clients_clustered['Cluster'] = clusters

                            st.success(f"‚úÖ {n_clusters} clusters identifi√©s")

                            # Distribution des clusters
                            st.subheader("üìä Distribution des clusters")

                            col_dist1, col_dist2 = st.columns(2)

                            with col_dist1:
                                cluster_counts = pd.Series(clusters).value_counts().sort_index()
                                fig_clusters = go.Figure(data=[go.Bar(
                                    x=cluster_counts.index,
                                    y=cluster_counts.values,
                                    marker_color='#3498DB',
                                    text=cluster_counts.values,
                                    textposition='auto'
                                )])
                                fig_clusters.update_layout(
                                    title=f'Distribution des {n_clusters} clusters',
                                    xaxis_title='Cluster',
                                    yaxis_title='Nombre de clients',
                                    height=400
                                )
                                st.plotly_chart(fig_clusters, use_container_width=True)

                            with col_dist2:
                                # Pie chart des clusters
                                fig_pie_clust = px.pie(
                                    values=cluster_counts.values,
                                    names=[f'Cluster {i}' for i in cluster_counts.index],
                                    title="R√©partition en %"
                                )
                                fig_pie_clust.update_layout(height=400)
                                st.plotly_chart(fig_pie_clust, use_container_width=True)

                            # Caract√©risation des clusters
                            st.subheader("üìä Caract√©risation des clusters")

                            # Adapter l'agr√©gation selon les colonnes disponibles
                            agg_dict = {}
                            if 'score_risque' in scored_clients_clustered.columns:
                                agg_dict['score_risque'] = 'mean'
                            if 'prime_totale' in scored_clients_clustered.columns:
                                agg_dict['prime_totale'] = 'mean'
                            if 'frequence_sinistre' in scored_clients_clustered.columns:
                                agg_dict['frequence_sinistre'] = 'mean'
                            if 'niveau_risque' in scored_clients_clustered.columns:
                                agg_dict['niveau_risque'] = lambda x: x.mode()[0] if not x.mode().empty else 'N/A'
                            if 'nomncli' in scored_clients_clustered.columns:
                                agg_dict['nomncli'] = 'count'

                            if agg_dict:
                                cluster_summary = scored_clients_clustered.groupby('Cluster').agg(agg_dict).round(2)

                                # Renommer les colonnes
                                rename_dict = {
                                    'score_risque': 'Score moyen',
                                    'prime_totale': 'Prime moyenne',
                                    'frequence_sinistre': 'Sinistres moyens',
                                    'niveau_risque': 'Risque dominant',
                                    'nomncli': 'Nombre clients'
                                }
                                cluster_summary = cluster_summary.rename(columns=rename_dict)

                                st.dataframe(
                                    cluster_summary.style.background_gradient(cmap='YlOrRd', axis=0),
                                    use_container_width=True
                                )

                            # Visualisation 2D des clusters
                            st.subheader("üó∫Ô∏è Visualisation des clusters")

                            if len(clustering_features) >= 2:
                                fig_scatter_clust = px.scatter(
                                    scored_clients_clustered,
                                    x=clustering_features[0],
                                    y=clustering_features[1],
                                    color='Cluster',
                                    title=f"Clusters projet√©s sur {clustering_features[0]} vs {clustering_features[1]}",
                                    height=500
                                )

                                # Ajouter les centres des clusters
                                centers = scaler.inverse_transform(kmeans.cluster_centers_)
                                fig_scatter_clust.add_trace(go.Scatter(
                                    x=centers[:, 0],
                                    y=centers[:, 1],
                                    mode='markers',
                                    marker=dict(
                                        size=20,
                                        color='red',
                                        symbol='x',
                                        line=dict(width=2, color='white')
                                    ),
                                    name='Centres'
                                ))

                                st.plotly_chart(fig_scatter_clust, use_container_width=True)

                            # Profils radar des clusters
                            st.subheader("üéØ Profils radar des clusters")

                            if len(clustering_features) >= 3:
                                fig_radar = go.Figure()

                                for cluster_id in range(n_clusters):
                                    cluster_data = scored_clients_clustered[
                                        scored_clients_clustered['Cluster'] == cluster_id]
                                    values = [cluster_data[feat].mean() for feat in clustering_features]

                                    # Normaliser entre 0 et 1
                                    values_norm = [(v - scored_clients_clustered[feat].min()) /
                                                   (scored_clients_clustered[feat].max() - scored_clients_clustered[
                                                       feat].min())
                                                   for v, feat in zip(values, clustering_features)]

                                    fig_radar.add_trace(go.Scatterpolar(
                                        r=values_norm + [values_norm[0]],
                                        theta=clustering_features + [clustering_features[0]],
                                        fill='toself',
                                        name=f'Cluster {cluster_id}'
                                    ))

                                fig_radar.update_layout(
                                    polar=dict(radialaxis=dict(visible=True, range=[0, 1])),
                                    showlegend=True,
                                    title="Profils moyens des clusters (normalis√©s)",
                                    height=500
                                )
                                st.plotly_chart(fig_radar, use_container_width=True)

                        except Exception as e:
                            st.error(f"‚ùå Erreur lors du clustering: {str(e)}")
                            st.code(traceback.format_exc())
            else:
                st.warning("‚ö†Ô∏è Pas assez de variables num√©riques pour le clustering")

        elif multivariate_choice == "Matrice de corr√©lation":
            st.subheader("üîó Matrice de corr√©lation compl√®te")

            # S√©lection des variables
            numeric_cols = scored_clients.select_dtypes(include=[np.number]).columns.tolist()

            selected_corr_vars = st.multiselect(
                "S√©lectionnez les variables pour la matrice de corr√©lation :",
                options=numeric_cols,
                default=numeric_cols[:min(10, len(numeric_cols))]
            )

            if len(selected_corr_vars) >= 2:
                # Calculer la matrice de corr√©lation
                corr_matrix = scored_clients[selected_corr_vars].corr()

                # Heatmap de corr√©lation
                fig_corr = px.imshow(
                    corr_matrix,
                    title="Matrice de corr√©lation",
                    color_continuous_scale='RdBu',
                    zmin=-1,
                    zmax=1,
                    aspect='auto'
                )
                fig_corr.update_layout(height=600)
                st.plotly_chart(fig_corr, use_container_width=True)

                # Top des corr√©lations
                st.subheader("üîù Top 15 des corr√©lations les plus fortes")

                # Extraire les corr√©lations
                correlations = corr_matrix.unstack()
                # Enlever les auto-corr√©lations
                correlations = correlations[correlations != 1]
                # Enlever les doublons
                correlations = correlations[correlations.index.get_level_values(0) < correlations.index.get_level_values(1)]
                # Trier par valeur absolue
                top_correlations = correlations.abs().sort_values(ascending=False).head(15)

                # Cr√©er un dataframe pour l'affichage
                corr_df = pd.DataFrame({
                    'Variable 1': [idx[0] for idx in top_correlations.index],
                    'Variable 2': [idx[1] for idx in top_correlations.index],
                    'Corr√©lation': [correlations[idx] for idx in top_correlations.index]
                })

                st.dataframe(
                    corr_df.style.background_gradient(cmap='RdBu', subset=['Corr√©lation'], vmin=-1, vmax=1),
                    use_container_width=True
                )

                # Graphique des top corr√©lations
                fig_top_corr = go.Figure(go.Bar(
                    x=corr_df['Corr√©lation'],
                    y=[f"{row['Variable 1']} - {row['Variable 2']}" for _, row in corr_df.iterrows()],
                    orientation='h',
                    marker_color=['red' if x < 0 else 'blue' for x in corr_df['Corr√©lation']]
                ))
                fig_top_corr.update_layout(
                    title="Top 15 des corr√©lations",
                    xaxis_title="Coefficient de corr√©lation",
                    height=500
                )
                st.plotly_chart(fig_top_corr, use_container_width=True)
        else:
            st.warning("‚ö†Ô∏è S√©lectionnez au moins 2 variables")

    # ============================================================
    # RAPPORT NARRATIF
    # ============================================================
    elif analysis_type == "üìÑ Rapport Narratif":
        st.subheader("üìÑ Rapport narratif pour d√©cideur")

        # G√©n√©rer le rapport complet
        report = insight_engine.generate_narrative_report(scored_clients)

        # Afficher avec un formatage propre
        st.markdown("### üìÑ Rapport d'Analyse Complet")
        st.markdown("---")
        st.markdown(report)

        # Options d'export
        st.markdown("---")
        st.subheader("üì§ Exporter le rapport")

        col1, col2 = st.columns(2)

        with col1:
            # Export en Markdown
            report_md = report
            st.download_button(
                label="üìÑ T√©l√©charger en Markdown",
                data=report_md,
                file_name="rapport_risque_clients.md",
                mime="text/markdown"
            )

        with col2:
            # Export des scores en CSV - adapter selon colonnes disponibles
            export_cols = ['nomncli', 'score_risque', 'niveau_risque']

            # Ajouter les colonnes optionnelles si elles existent
            optional_cols = ['prime_totale', 'frequence_sinistre', 'insight']
            for col in optional_cols:
                if col in scored_clients.columns:
                    export_cols.append(col)

            csv_data = scored_clients[export_cols].to_csv(index=False, encoding='utf-8-sig')

            st.download_button(
                label="üìä T√©l√©charger les scores",
                data=csv_data,
                file_name="scores_risque_clients.csv",
                mime="text/csv"
            )
# ============================================================
# 4Ô∏è‚É£ MOD√àLES PR√âDICTIFS AVANC√âS - VERSION AVEC S√âLECTION DE VARIABLES
# ============================================================
elif page == " üßÆ Mod√®les Pr√©dictifs":
    st.header("üéØ Mod√©lisation Pr√©dictive Avanc√©e")

    if not st.session_state.data_loaded:
        st.warning("‚ö†Ô∏è Veuillez d'abord charger des donn√©es")
        st.stop()

    # Utiliser les donn√©es pr√©par√©es si disponibles, sinon les donn√©es brutes
    if st.session_state.df_final is not None:
        df_final = st.session_state.df_final
    else:
        df_final = st.session_state.dataframe.copy()

    # Initialisation du moteur pr√©dictif avanc√©
    if st.session_state.predictive_engine is None:
        try:
            try:
                from modules.predictive_engine import AdvancedPredictiveEngine
            except ImportError:
                sys.path.append(os.path.join(BASE_DIR, "modules"))
                from predictive_engine import AdvancedPredictiveEngine

            st.session_state.predictive_engine = AdvancedPredictiveEngine()
            st.success("‚úÖ Moteur pr√©dictif avanc√© initialis√©")
        except ImportError as e:
            st.error(f"‚ùå Impossible d'importer le module predictive_engine: {e}")
            st.info("""
            **Assurez-vous que:**
            1. Le fichier `predictive_engine.py` est dans le dossier `modules/`
            2. Les d√©pendances sont install√©es:
            ```bash
            pip install scikit-learn statsmodels prophet xgboost catboost plotly
            ```
            """)
            st.stop()

    predictive_engine = st.session_state.predictive_engine

    # Onglets pour diff√©rents types de mod√©lisation
    tab1, tab2, tab3 = st.tabs([
        "üîç Analyse Exploratoire",
        "üéØ Classification",
        "üìà R√©gression"
    ])

    # ============================================================
    # TAB 1: ANALYSE EXPLORATOIRE
    # ============================================================
    with tab1:
        st.subheader("üîç Analyse Exploratoire des Donn√©es")

        # S√©lection de la variable cible pour l'exploration
        col_detect1, col_detect2 = st.columns(2)

        with col_detect1:
            all_columns = list(df_final.columns)
            target_col = st.selectbox(
                "Variable cible (target)",
                options=["Aucune"] + all_columns,
                help="S√©lectionnez la variable √† analyser",
                key="target_col_exploratory"
            )

        with col_detect2:
            if target_col != "Aucune" and st.button("üîç Analyser la variable", type="primary"):
                with st.spinner("Analyse en cours..."):
                    y_series = df_final[target_col]
                    n_unique = y_series.nunique()

                    if n_unique == 2:
                        st.success("‚úÖ Classification binaire d√©tect√©e")
                        st.info(f"Distribution: {y_series.value_counts().to_dict()}")

                        # Graphique de distribution
                        fig = px.pie(
                            values=y_series.value_counts().values,
                            names=y_series.value_counts().index,
                            title=f"Distribution de {target_col}"
                        )
                        st.plotly_chart(fig, use_container_width=True)

                    elif 2 < n_unique <= 10:
                        st.success("‚úÖ Classification multi-classes d√©tect√©e")
                        st.info(f"{n_unique} classes d√©tect√©es")

                        # Graphique en barres
                        fig = px.bar(
                            x=y_series.value_counts().index,
                            y=y_series.value_counts().values,
                            title=f"Distribution de {target_col}",
                            labels={'x': 'Classe', 'y': 'Nombre'}
                        )
                        st.plotly_chart(fig, use_container_width=True)

                    elif pd.api.types.is_numeric_dtype(y_series):
                        st.success("‚úÖ Variable num√©rique continue")

                        # Statistiques
                        col_stat1, col_stat2, col_stat3, col_stat4 = st.columns(4)
                        with col_stat1:
                            st.metric("Moyenne", f"{y_series.mean():.2f}")
                        with col_stat2:
                            st.metric("√âcart-type", f"{y_series.std():.2f}")
                        with col_stat3:
                            st.metric("Min", f"{y_series.min():.2f}")
                        with col_stat4:
                            st.metric("Max", f"{y_series.max():.2f}")

                        # Histogramme
                        fig = px.histogram(
                            df_final,
                            x=target_col,
                            title=f"Distribution de {target_col}",
                            nbins=50
                        )
                        st.plotly_chart(fig, use_container_width=True)

        # Analyse des corr√©lations
        st.subheader("üìä Analyse des Corr√©lations")

        # S√©lection des variables pour l'analyse de corr√©lation
        numeric_cols = df_final.select_dtypes(include=[np.number]).columns.tolist()

        if len(numeric_cols) > 1:
            selected_corr_vars = st.multiselect(
                "S√©lectionnez les variables pour l'analyse de corr√©lation:",
                options=numeric_cols,
                default=numeric_cols[:min(10, len(numeric_cols))],
                help="S√©lectionnez les variables num√©riques √† analyser"
            )

            if len(selected_corr_vars) > 1:
                if st.button("üîó Calculer les corr√©lations", type="primary"):
                    # Calculer la matrice de corr√©lation
                    corr_matrix = df_final[selected_corr_vars].corr()

                    # Heatmap de corr√©lation
                    fig = px.imshow(
                        corr_matrix,
                        title="Matrice de Corr√©lation",
                        color_continuous_scale='RdBu',
                        zmin=-1,
                        zmax=1
                    )
                    st.plotly_chart(fig, use_container_width=True)

                    # Top des corr√©lations
                    st.markdown("**Top 10 des corr√©lations les plus fortes:**")
                    correlations = corr_matrix.unstack().sort_values(ascending=False)
                    # Enlever les auto-corr√©lations (valeur 1)
                    correlations = correlations[correlations != 1]

                    top_correlations = correlations.head(10)
                    for idx, value in top_correlations.items():
                        var1, var2 = idx
                        st.info(f"{var1} ‚Üî {var2}: **{value:.3f}**")
        else:
            st.warning("‚ö†Ô∏è Pas assez de colonnes num√©riques pour l'analyse de corr√©lation")

    # ============================================================
    # IMPORTATIONS N√âCESSAIRES
    # ============================================================

    import pandas as pd
    import numpy as np
    import streamlit as st
    import plotly.express as px
    import plotly.graph_objects as go
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler
    from sklearn.impute import SimpleImputer
    from sklearn.feature_selection import VarianceThreshold
    import traceback
    from datetime import datetime


    # ============================================================
    # FONCTION UTILITAIRE AM√âLIOR√âE POUR PR√âPARER LES DONN√âES
    # ============================================================

    def prepare_data_for_ml(df, target_col, selected_features, test_size=0.2, handle_missing="imputer_median",
                            scale_data=True, task_type="classification"):
        """
        Pr√©pare les donn√©es pour le machine learning avec gestion robuste des types de donn√©es
        """
        try:
            # 1. VALIDATION DES ENTR√âES
            if target_col not in df.columns:
                raise ValueError(f"La variable cible '{target_col}' n'existe pas dans le DataFrame")

            missing_features = [f for f in selected_features if f not in df.columns]
            if missing_features:
                selected_features = [f for f in selected_features if f in df.columns]
                st.warning(f"‚ö†Ô∏è Features manquantes exclues: {missing_features}")

            # 2. S√âLECTION DES DONN√âES
            features_to_use = [f for f in selected_features if f != target_col and f in df.columns]
            if not features_to_use:
                raise ValueError("Aucune feature valide s√©lectionn√©e")

            df_selected = df[[target_col] + features_to_use].copy()

            # 3. EXCLUSION DES COLONNES NON UTILISABLES
            # Exclure les dates, timedelta, et types complexes
            date_cols = []
            for col in features_to_use:
                col_dtype = str(df_selected[col].dtype)
                if 'datetime' in col_dtype or 'timedelta' in col_dtype:
                    date_cols.append(col)

            if date_cols:
                df_selected = df_selected.drop(columns=date_cols)
                st.warning(f"‚ö†Ô∏è Colonnes de type date exclues: {date_cols}")

            # Mettre √† jour les features apr√®s exclusion
            features_to_use = [col for col in df_selected.columns if col != target_col]

            # 4. S√âPARATION FEATURES/TARGET
            X = df_selected[features_to_use]
            y = df_selected[target_col]

            # 5. ENCODAGE DES VARIABLES CAT√âGORIELLES
            # Identifier les colonnes cat√©gorielles restantes
            cat_cols = []
            for col in X.columns:
                if X[col].dtype == 'object' or (hasattr(X[col], 'dtype') and X[col].dtype.name == 'category'):
                    unique_vals = X[col].nunique()
                    if unique_vals <= 50:  # Limite pour √©viter l'explosion dimensionnelle
                        cat_cols.append(col)
                    else:
                        X = X.drop(columns=[col])
                        st.warning(f"‚ö†Ô∏è Colonne '{col}' exclue (trop de valeurs uniques: {unique_vals})")

            if cat_cols:
                X_encoded = pd.get_dummies(X, columns=cat_cols, drop_first=True, dtype=int)
            else:
                X_encoded = X.copy()

            # 6. CONVERSION DES BOOL√âENS
            bool_cols = X_encoded.select_dtypes(include=['bool']).columns
            X_encoded[bool_cols] = X_encoded[bool_cols].astype(int)

            # 7. GESTION DES VALEURS MANQUANTES
            numeric_cols = X_encoded.select_dtypes(include=[np.number]).columns

            if handle_missing == "imputer_median" and len(numeric_cols) > 0:
                imputer = SimpleImputer(strategy='median')
                X_encoded[numeric_cols] = imputer.fit_transform(X_encoded[numeric_cols])
            elif handle_missing == "imputer_mean" and len(numeric_cols) > 0:
                imputer = SimpleImputer(strategy='mean')
                X_encoded[numeric_cols] = imputer.fit_transform(X_encoded[numeric_cols])
            elif handle_missing == "drop":
                mask = X_encoded.notna().all(axis=1) & y.notna()
                X_encoded = X_encoded[mask]
                y = y[mask]
            else:
                # Par d√©faut, imputer avec la m√©diane
                if len(numeric_cols) > 0:
                    imputer = SimpleImputer(strategy='median')
                    X_encoded[numeric_cols] = imputer.fit_transform(X_encoded[numeric_cols])

            # 8. V√âRIFICATION ET NETTOYAGE FINAL
            # Supprimer les colonnes avec variance nulle
            if len(X_encoded.columns) > 0:
                selector = VarianceThreshold(threshold=0.01)
                try:
                    X_encoded_arr = selector.fit_transform(X_encoded)
                    kept_indices = selector.get_support(indices=True)
                    X_encoded = X_encoded.iloc[:, kept_indices]
                    if len(kept_indices) < X_encoded.shape[1]:
                        st.info(f"üìâ {X_encoded.shape[1] - len(kept_indices)} colonnes √† faible variance supprim√©es")
                except Exception as e:
                    st.warning(f"‚ö†Ô∏è Impossible d'appliquer le filtre de variance: {str(e)}")

            # 9. NORMALISATION
            if scale_data and len(X_encoded.columns) > 0:
                scaler = StandardScaler()
                X_scaled = scaler.fit_transform(X_encoded)
                X_encoded = pd.DataFrame(
                    X_scaled,
                    columns=X_encoded.columns,
                    index=X_encoded.index
                )

            # 10. SPLIT DES DONN√âES AVEC STRATIFICATION INTELLIGENTE
            if len(X_encoded) == 0:
                raise ValueError("Aucune donn√©e valide apr√®s pr√©traitement")

            if task_type == "classification":
                # V√©rifier si la stratification est possible
                y_unique = y.nunique()
                if y_unique >= 2:
                    class_counts = y.value_counts()
                    min_class_size = class_counts.min()

                    # Stratification seulement si toutes les classes ont au moins 2 √©chantillons
                    if min_class_size >= 2 and y_unique <= 20:  # Limite pour √©viter trop de classes
                        try:
                            X_train, X_test, y_train, y_test = train_test_split(
                                X_encoded, y, test_size=test_size,
                                random_state=42, stratify=y
                            )
                            st.success("‚úÖ Stratification appliqu√©e avec succ√®s")
                        except Exception as e:
                            st.warning(f"‚ö†Ô∏è Stratification √©chou√©e: {str(e)}. Utilisation sans stratification.")
                            X_train, X_test, y_train, y_test = train_test_split(
                                X_encoded, y, test_size=test_size,
                                random_state=42
                            )
                    else:
                        X_train, X_test, y_train, y_test = train_test_split(
                            X_encoded, y, test_size=test_size,
                            random_state=42
                        )
                        if min_class_size < 2:
                            st.warning("‚ö†Ô∏è Stratification d√©sactiv√©e (certaines classes ont < 2 √©chantillons)")
                else:
                    X_train, X_test, y_train, y_test = train_test_split(
                        X_encoded, y, test_size=test_size,
                        random_state=42
                    )
            else:  # R√©gression
                X_train, X_test, y_train, y_test = train_test_split(
                    X_encoded, y, test_size=test_size,
                    random_state=42
                )

            # V√©rification finale
            if X_train.shape[1] == 0:
                raise ValueError("Aucune feature valide apr√®s pr√©traitement")

            return X_train, X_test, y_train, y_test, X_encoded.columns.tolist()

        except Exception as e:
            st.error(f"‚ùå Erreur dans prepare_data_for_ml: {str(e)}")
            raise


    # ============================================================
    # FONCTION POUR FILTRER LES COLONNES UTILISABLES
    # ============================================================

    def get_usable_columns(df, exclude_dates=True, max_categories=20):
        """
        Retourne les colonnes utilisables pour le ML
        """
        usable_cols = []

        for col in df.columns:
            # Exclure les dates si demand√©
            if exclude_dates:
                col_dtype = str(df[col].dtype)
                if 'datetime' in col_dtype or 'timedelta' in col_dtype:
                    continue

            # Pour les colonnes cat√©gorielles, v√©rifier le nombre de valeurs uniques
            if df[col].dtype == 'object' or (hasattr(df[col], 'dtype') and df[col].dtype.name == 'category'):
                unique_count = df[col].nunique()
                if unique_count <= max_categories:
                    usable_cols.append(col)
                else:
                    # V√©rifier si c'est vraiment num√©rique mais stock√© comme object
                    try:
                        pd.to_numeric(df[col], errors='raise')
                        usable_cols.append(col)
                    except:
                        continue
            else:
                usable_cols.append(col)

        return usable_cols


    # ============================================================
    # TAB 2: CLASSIFICATION OPTIMIS√âE
    # ============================================================
    with tab2:
        st.subheader("üéØ Classification Supervis√©e")

        # Initialisation des variables
        target_col = None
        selected_features = []

        # Section 1: S√©lection des variables
        st.markdown("### üîß Configuration du probl√®me")

        col_sel1, col_sel2 = st.columns(2)

        with col_sel1:
            # Obtenir les colonnes utilisables
            usable_cols = get_usable_columns(df_final, exclude_dates=True, max_categories=20)

            # Variables cibles potentielles (cat√©gorielles avec nombre raisonnable de classes)
            target_candidates = []
            for col in usable_cols:
                unique_count = df_final[col].nunique()
                if 2 <= unique_count <= 20:
                    target_candidates.append((col, unique_count))

            # Trier par nombre de classes
            target_candidates.sort(key=lambda x: x[1])
            target_options = [col for col, _ in target_candidates]

            if not target_options:
                st.error("‚ùå Aucune variable cible cat√©gorielle valide trouv√©e")
                st.stop()

            target_col = st.selectbox(
                "üéØ Variable cible:",
                options=target_options,
                help="Variable cat√©gorielle √† pr√©dire (2-20 classes)",
                key="target_col_class"
            )

            # Afficher les statistiques de la cible
            if target_col:
                col_stats1, col_stats2, col_stats3 = st.columns(3)
                with col_stats1:
                    st.metric("Classes", df_final[target_col].nunique())
                with col_stats2:
                    missing = df_final[target_col].isna().sum()
                    st.metric("Manquantes", missing)
                with col_stats3:
                    total = len(df_final)
                    st.metric("Total", total)

                # Distribution des classes
                class_dist = df_final[target_col].value_counts().head(10)
                if len(class_dist) > 0:
                    fig = px.bar(
                        x=class_dist.index.astype(str),
                        y=class_dist.values,
                        title=f"Top 10 classes de '{target_col}'",
                        labels={'x': 'Classe', 'y': 'Nombre'},
                        color=class_dist.values,
                        color_continuous_scale='Blues'
                    )
                    st.plotly_chart(fig, use_container_width=True)

        with col_sel2:
            # Variables pr√©dictives (exclure la cible)
            if target_col:
                feature_candidates = [col for col in usable_cols if col != target_col]
            else:
                feature_candidates = usable_cols

            st.markdown("**üìä Variables pr√©dictives:**")

            # Option de s√©lection rapide
            col_opt1, col_opt2 = st.columns(2)
            with col_opt1:
                use_all = st.checkbox("Toutes les variables", value=True, key="use_all_class")
            with col_opt2:
                use_numeric_only = st.checkbox("Num√©riques seulement", value=False, key="numeric_only_class")

            if use_numeric_only:
                feature_candidates = [col for col in feature_candidates
                                      if pd.api.types.is_numeric_dtype(df_final[col])]

            if not use_all:
                selected_features = st.multiselect(
                    "S√©lectionnez les features:",
                    options=feature_candidates,
                    default=feature_candidates[:min(10, len(feature_candidates))],
                    help="Choisissez les variables pour pr√©dire la cible",
                    key="features_class"
                )
            else:
                selected_features = feature_candidates

            # Aper√ßu des features s√©lectionn√©es
            if selected_features:
                st.info(f"‚úÖ {len(selected_features)} features s√©lectionn√©es")

                # Types de donn√©es
                numeric_count = sum(1 for f in selected_features
                                    if pd.api.types.is_numeric_dtype(df_final[f]))
                cat_count = len(selected_features) - numeric_count

                col_type1, col_type2, col_type3 = st.columns(3)
                with col_type1:
                    st.metric("Num√©riques", numeric_count)
                with col_type2:
                    st.metric("Cat√©gorielles", cat_count)
                with col_type3:
                    # Estimation de la dimension apr√®s one-hot encoding
                    estimated_dim = numeric_count
                    for f in selected_features:
                        if not pd.api.types.is_numeric_dtype(df_final[f]):
                            unique_count = min(df_final[f].nunique(), 10)
                            estimated_dim += unique_count - 1
                    st.metric("Dim. estim√©e", estimated_dim)

        # Section 2: Configuration avanc√©e
        st.markdown("### ‚öôÔ∏è Param√®tres avanc√©s")

        col_adv1, col_adv2, col_adv3 = st.columns(3)

        with col_adv1:
            model_type = st.selectbox(
                "ü§ñ Algorithme:",
                options=["random_forest", "xgboost", "logistic_regression", "gradient_boosting"],
                help="S√©lectionnez l'algorithme de classification",
                key="model_type_class"
            )

            test_size = st.slider(
                "üìä Taille test (%):",
                min_value=10, max_value=40, value=20,
                help="Pourcentage pour la validation",
                key="test_size_class"
            ) / 100

        with col_adv2:
            handle_missing = st.selectbox(
                "üîÑ Valeurs manquantes:",
                options=["imputer_median", "imputer_mean", "drop"],
                help="Strat√©gie de gestion des NaN",
                key="handle_missing_class"
            )

            scale_features = st.selectbox(
                "üìè Normalisation:",
                options=["standard", "minmax", "aucune"],
                help="M√©thode de normalisation",
                key="scale_class"
            )

        with col_adv3:
            optimize_params = st.checkbox("üîç Optimisation hyperparam√®tres", value=True, key="optimize_class")
            cv_folds = st.slider("üåÄ Validation crois√©e:", 2, 10, 5, key="cv_class")
            balance_classes = st.checkbox("‚öñÔ∏è √âquilibrer les classes", value=False, key="balance_class")

        # Section 3: Entra√Ænement
        st.markdown("### üöÄ Entra√Ænement du mod√®le")

        if not target_col or not selected_features:
            st.warning("‚ö†Ô∏è S√©lectionnez une variable cible et des features")
        else:
            # R√©sum√© de la configuration
            with st.expander("üìÑ R√©sum√© de la configuration", expanded=True):
                col_sum1, col_sum2, col_sum3 = st.columns(3)
                with col_sum1:
                    st.metric("Cible", target_col)
                    st.metric("Classes", df_final[target_col].nunique())
                with col_sum2:
                    st.metric("Features", len(selected_features))
                    st.metric("√âchantillons", len(df_final))
                with col_sum3:
                    st.metric("Test size", f"{test_size * 100:.0f}%")
                    st.metric("Mod√®le", model_type)

            col_train1, col_train2, col_train3 = st.columns([1, 2, 1])
            with col_train2:
                train_button = st.button(
                    "üéØ D√©marrer l'entra√Ænement",
                    type="primary",
                    use_container_width=True,
                    key="train_button_class"
                )

        # PROCESSUS D'ENTRA√éNEMENT
        if train_button and target_col and selected_features:
            try:
                # Pr√©paration des donn√©es
                with st.spinner("üîÑ Pr√©paration des donn√©es en cours..."):
                    # Adapter scale_data pour la nouvelle fonction
                    scale_bool = scale_features != "aucune"

                    X_train, X_test, y_train, y_test, feature_names = prepare_data_for_ml(
                        df_final,
                        target_col,
                        selected_features,
                        test_size=test_size,
                        handle_missing=handle_missing,
                        scale_data=scale_bool,
                        task_type="classification"
                    )

                    # V√©rifications finales
                    if X_train.shape[1] == 0:
                        st.error("‚ùå Aucune feature valide apr√®s pr√©traitement")
                        st.stop()

                    if len(y_train) == 0:
                        st.error("‚ùå Aucun √©chantillon d'entra√Ænement valide")
                        st.stop()

                    # Afficher les informations
                    st.success("‚úÖ Donn√©es pr√©par√©es avec succ√®s")

                    info_col1, info_col2, info_col3, info_col4 = st.columns(4)
                    with info_col1:
                        st.metric("Train", f"{len(X_train):,}")
                    with info_col2:
                        st.metric("Test", f"{len(X_test):,}")
                    with info_col3:
                        st.metric("Features", X_train.shape[1])
                    with info_col4:
                        class_balance = y_train.value_counts().min() / y_train.value_counts().max()
                        st.metric("Balance", f"{class_balance:.2%}")

                # ENTRA√éNEMENT DU MOD√àLE
                with st.spinner(f"üéØ Entra√Ænement du mod√®le {model_type}..."):
                    # Import des mod√®les de classification
                    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
                    from sklearn.linear_model import LogisticRegression
                    from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                                                 f1_score, confusion_matrix, classification_report,
                                                 roc_auc_score, roc_curve)

                    # Initialisation du mod√®le selon le type
                    if model_type == "random_forest":
                        model = RandomForestClassifier(
                            n_estimators=100,
                            max_depth=10,
                            min_samples_split=5,
                            min_samples_leaf=2,
                            random_state=42,
                            n_jobs=-1,
                            class_weight='balanced' if balance_classes else None
                        )

                    elif model_type == "xgboost":
                        try:
                            from xgboost import XGBClassifier

                            model = XGBClassifier(
                                n_estimators=100,
                                max_depth=6,
                                learning_rate=0.1,
                                random_state=42,
                                use_label_encoder=False,
                                eval_metric='logloss'
                            )
                        except ImportError:
                            st.warning("XGBoost non install√©, utilisation de Random Forest")
                            model = RandomForestClassifier(n_estimators=100, random_state=42)

                    elif model_type == "gradient_boosting":
                        model = GradientBoostingClassifier(
                            n_estimators=100,
                            learning_rate=0.1,
                            max_depth=3,
                            random_state=42
                        )

                    elif model_type == "logistic_regression":
                        model = LogisticRegression(
                            max_iter=1000,
                            random_state=42,
                            class_weight='balanced' if balance_classes else None,
                            solver='lbfgs'
                        )

                    else:
                        model = RandomForestClassifier(n_estimators=100, random_state=42)

                    # Optimisation des hyperparam√®tres si demand√©e
                    if optimize_params:
                        with st.spinner("üîç Optimisation des hyperparam√®tres..."):
                            from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

                            if model_type == "random_forest":
                                param_grid = {
                                    'n_estimators': [50, 100, 200],
                                    'max_depth': [5, 10, 15, None],
                                    'min_samples_split': [2, 5, 10],
                                    'min_samples_leaf': [1, 2, 4]
                                }
                                search = GridSearchCV(
                                    model, param_grid,
                                    cv=min(cv_folds, 5),  # Limiter √† 5 folds pour la performance
                                    scoring='f1_weighted',
                                    n_jobs=-1,
                                    verbose=0
                                )

                            elif model_type == "xgboost":
                                param_grid = {
                                    'n_estimators': [50, 100, 150],
                                    'max_depth': [3, 6, 9],
                                    'learning_rate': [0.01, 0.1, 0.3],
                                    'subsample': [0.8, 1.0]
                                }
                                search = RandomizedSearchCV(
                                    model, param_grid,
                                    n_iter=10,
                                    cv=min(cv_folds, 5),
                                    scoring='f1_weighted',
                                    n_jobs=-1,
                                    random_state=42,
                                    verbose=0
                                )

                            elif model_type == "logistic_regression":
                                param_grid = {
                                    'C': [0.01, 0.1, 1, 10, 100],
                                    'penalty': ['l2'],
                                    'solver': ['lbfgs', 'liblinear']
                                }
                                search = GridSearchCV(
                                    model, param_grid,
                                    cv=min(cv_folds, 5),
                                    scoring='f1_weighted',
                                    n_jobs=-1,
                                    verbose=0
                                )

                            else:
                                # Pas d'optimisation pour les autres mod√®les
                                search = None

                            if search is not None:
                                search.fit(X_train, y_train)
                                model = search.best_estimator_
                                st.success(f"‚úÖ Meilleurs param√®tres: {search.best_params_}")

                    # Entra√Ænement du mod√®le
                    model.fit(X_train, y_train)

                    # Pr√©dictions
                    y_pred = model.predict(X_test)
                    y_pred_proba = model.predict_proba(X_test) if hasattr(model, "predict_proba") else None

                    # Sauvegarde dans session state
                    st.session_state.classification_model = {
                        'model': model,
                        'features': feature_names,
                        'target': target_col,
                        'model_type': model_type,
                        'X_train': X_train,
                        'X_test': X_test,
                        'y_train': y_train,
                        'y_test': y_test,
                        'y_pred': y_pred,
                        'y_pred_proba': y_pred_proba
                    }

                st.success("‚úÖ Mod√®le entra√Æn√© avec succ√®s !")

                # ============================================
                # AFFICHAGE DES R√âSULTATS
                # ============================================

                # 1. M√©triques de performance
                st.subheader("üìä Performance du mod√®le")

                # Calcul des m√©triques
                accuracy = accuracy_score(y_test, y_pred)
                precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
                recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
                f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)

                # Affichage des m√©triques
                col_met1, col_met2, col_met3, col_met4 = st.columns(4)
                with col_met1:
                    st.metric("Accuracy", f"{accuracy:.3f}")
                with col_met2:
                    st.metric("Precision", f"{precision:.3f}")
                with col_met3:
                    st.metric("Recall", f"{recall:.3f}")
                with col_met4:
                    st.metric("F1-Score", f"{f1:.3f}")

                # 2. Matrice de confusion
                st.subheader("üìÑ Matrice de confusion")

                cm = confusion_matrix(y_test, y_pred)
                class_labels = sorted(y_test.unique())

                # Cr√©ation de la heatmap
                fig_cm = px.imshow(
                    cm,
                    labels=dict(x="Pr√©dit", y="R√©el", color="Count"),
                    x=[f'Pr√©dit {label}' for label in class_labels],
                    y=[f'R√©el {label}' for label in class_labels],
                    text_auto=True,
                    color_continuous_scale='Blues',
                    aspect="auto"
                )
                fig_cm.update_layout(title="Matrice de Confusion")
                st.plotly_chart(fig_cm, use_container_width=True)

                # 3. Rapport de classification d√©taill√©
                with st.expander("üìÑ Rapport de classification d√©taill√©"):
                    report = classification_report(y_test, y_pred, output_dict=True)
                    report_df = pd.DataFrame(report).transpose()
                    st.dataframe(report_df.style.format("{:.3f}").background_gradient(cmap='Blues'),
                                 use_container_width=True)

                # 4. Importance des features
                if hasattr(model, 'feature_importances_'):
                    st.subheader("üéØ Importance des features")

                    # Cr√©ation du dataframe d'importance
                    importance_df = pd.DataFrame({
                        'Feature': feature_names[:len(model.feature_importances_)],
                        'Importance': model.feature_importances_
                    }).sort_values('Importance', ascending=False)

                    # Graphique des top 15 features
                    fig_importance = px.bar(
                        importance_df.head(15),
                        x='Importance',
                        y='Feature',
                        orientation='h',
                        title='Top 15 des features les plus importantes',
                        color='Importance',
                        color_continuous_scale='Viridis'
                    )
                    fig_importance.update_layout(yaxis={'categoryorder': 'total ascending'})
                    st.plotly_chart(fig_importance, use_container_width=True)

                    # Tableau complet des importances
                    with st.expander("üìÑ Voir toutes les importances"):
                        st.dataframe(importance_df, use_container_width=True)


                # 6. Pr√©dictions d√©taill√©es
                st.subheader("üîÆ Pr√©dictions sur le jeu de test")

                if st.button("üìä Afficher les pr√©dictions d√©taill√©es", key="show_preds_class"):
                    results_df = pd.DataFrame({
                        'Vraie_valeur': y_test.values,
                        'Pr√©diction': y_pred
                    })

                    if y_pred_proba is not None:
                        for i, class_label in enumerate(sorted(y_test.unique())):
                            results_df[f'Probabilit√©_Classe_{class_label}'] = y_pred_proba[:, i]

                    results_df['Correct'] = results_df['Vraie_valeur'] == results_df['Pr√©diction']

                    # Affichage avec coloration
                    st.dataframe(
                        results_df.head(50).style.apply(
                            lambda x: ['background-color: #d4edda' if x['Correct'] else 'background-color: #f8d7da' for
                                       _ in x],
                            axis=1
                        ),
                        use_container_width=True
                    )

                    # T√©l√©chargement
                    csv = results_df.to_csv(index=False)
                    st.download_button(
                        label="üì• T√©l√©charger toutes les pr√©dictions",
                        data=csv,
                        file_name=f"predictions_classification_{target_col}_{model_type}.csv",
                        mime="text/csv",
                        key="download_preds_class"
                    )

                # 7. Export du mod√®le
                st.markdown("---")
                st.subheader("üíæ Export du mod√®le")

                col_exp1, col_exp2 = st.columns(2)

                with col_exp1:
                    if st.button("üíæ Sauvegarder le mod√®le", key="save_model_class"):
                        import joblib
                        import os

                        os.makedirs("models", exist_ok=True)
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        model_path = f"models/classification_{target_col}_{model_type}_{timestamp}.pkl"

                        model_data = {
                            'model': model,
                            'features': feature_names,
                            'target': target_col,
                            'model_type': model_type,
                            'accuracy': accuracy,
                            'metadata': {
                                'training_date': timestamp,
                                'n_samples_train': len(X_train),
                                'n_samples_test': len(X_test),
                                'n_features': len(feature_names),
                                'test_size': test_size
                            }
                        }

                        joblib.dump(model_data, model_path)
                        st.success(f"‚úÖ Mod√®le sauvegard√©: `{model_path}`")

                with col_exp2:
                    # Code pour reproduire le mod√®le
                    with st.expander("üìù Code de reproduction"):
                        st.code(f"""
    # Code pour reproduire le mod√®le {model_type}
    import pandas as pd
    import numpy as np
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler

    # Charger les donn√©es
    # df = pd.read_csv('votre_fichier.csv')

    # S√©lectionner les m√™mes variables
    X = df[{selected_features}]
    y = df['{target_col}']

    # Encodage one-hot pour les variables cat√©gorielles
    X = pd.get_dummies(X, drop_first=True)

    # Imputation des valeurs manquantes
    from sklearn.impute import SimpleImputer
    imputer = SimpleImputer(strategy='median')
    X = imputer.fit_transform(X)

    # Normalisation
    scaler = StandardScaler()
    X = scaler.fit_transform(X)

    # Split des donn√©es
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size={test_size}, random_state=42
    )

    # Entra√Æner le mod√®le
    from sklearn.ensemble import RandomForestClassifier
    model = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        random_state=42
    )
    model.fit(X_train, y_train)

    # √âvaluation
    y_pred = model.predict(X_test)
    accuracy = (y_pred == y_test).mean()
    print(f"Accuracy: {{accuracy:.3f}}")
                        """)

            except Exception as e:
                st.error(f"‚ùå Erreur lors de l'entra√Ænement: {str(e)}")
                st.code(traceback.format_exc())

    # ============================================================
    # TAB 3: R√âGRESSION OPTIMIS√âE
    # ============================================================
    with tab3:
        st.subheader("üìà R√©gression Supervis√©e")

        # Initialisation des variables
        target_col_reg = None
        selected_features_reg = []

        # Section 1: S√©lection des variables
        st.markdown("### üîß Configuration du probl√®me")

        col_sel1, col_sel2 = st.columns(2)

        with col_sel1:
            # Colonnes utilisables pour la r√©gression
            usable_cols_reg = get_usable_columns(df_final, exclude_dates=True)

            # Variables cibles num√©riques
            numeric_targets = []
            for col in usable_cols_reg:
                try:
                    if pd.api.types.is_numeric_dtype(df_final[col]):
                        # V√©rifier qu'il y a suffisamment de valeurs uniques
                        if df_final[col].nunique() > 5:  # √âviter les variables quasi-cat√©gorielles
                            numeric_targets.append(col)
                except:
                    continue

            if not numeric_targets:
                st.error("‚ùå Aucune variable num√©rique valide pour la r√©gression")
                st.stop()

            target_col_reg = st.selectbox(
                "üéØ Variable cible:",
                options=numeric_targets,
                help="Variable num√©rique continue √† pr√©dire",
                key="target_col_reg"
            )

            # Statistiques de la cible
            if target_col_reg:
                target_stats = df_final[target_col_reg].describe()

                col_stat1, col_stat2, col_stat3, col_stat4 = st.columns(4)
                with col_stat1:
                    st.metric("Moyenne", f"{target_stats['mean']:.2f}")
                with col_stat2:
                    st.metric("Std", f"{target_stats['std']:.2f}")
                with col_stat3:
                    st.metric("Min", f"{target_stats['min']:.2f}")
                with col_stat4:
                    st.metric("Max", f"{target_stats['max']:.2f}")

                # Distribution
                fig = px.histogram(df_final, x=target_col_reg,
                                   title=f"Distribution de '{target_col_reg}'",
                                   nbins=50,
                                   color_discrete_sequence=['#636EFA'])
                st.plotly_chart(fig, use_container_width=True)

        with col_sel2:
            # Variables pr√©dictives
            if target_col_reg:
                feature_candidates = [col for col in usable_cols_reg if col != target_col_reg]
            else:
                feature_candidates = usable_cols_reg

            st.markdown("**üìä Variables pr√©dictives:**")

            # Options de filtrage
            col_filt1, col_filt2 = st.columns(2)
            with col_filt1:
                use_all_reg = st.checkbox("Toutes les variables", value=True, key="use_all_reg")
            with col_filt2:
                filter_correlation = st.checkbox("Filtrer par corr√©lation", value=False, key="filter_corr")

            if not use_all_reg:
                selected_features_reg = st.multiselect(
                    "S√©lectionnez les features:",
                    options=feature_candidates,
                    default=feature_candidates[:min(10, len(feature_candidates))],
                    help="Choisissez les pr√©dicteurs",
                    key="features_reg"
                )
            else:
                selected_features_reg = feature_candidates

            # Filtrage par corr√©lation si activ√©
            if filter_correlation and target_col_reg and selected_features_reg:
                numeric_features = [f for f in selected_features_reg
                                    if pd.api.types.is_numeric_dtype(df_final[f])]
                if numeric_features:
                    corr_values = []
                    for feat in numeric_features:
                        try:
                            corr = df_final[[target_col_reg, feat]].corr().iloc[0, 1]
                            if not pd.isna(corr):
                                corr_values.append((feat, abs(corr)))
                        except:
                            pass

                    # Garder les features les plus corr√©l√©es
                    if corr_values:
                        corr_values.sort(key=lambda x: x[1], reverse=True)
                        top_features = [f[0] for f in corr_values[:20]]  # Top 20
                        selected_features_reg = top_features
                        st.success(f"‚úÖ {len(selected_features_reg)} features s√©lectionn√©es par corr√©lation")

        # Section 2: Configuration
        st.markdown("### ‚öôÔ∏è Param√®tres avanc√©s")

        col_conf1, col_conf2, col_conf3 = st.columns(3)

        with col_conf1:
            model_type_reg = st.selectbox(
                "ü§ñ Algorithme:",
                options=["random_forest", "xgboost", "linear_regression", "gradient_boosting"],
                help="S√©lectionnez l'algorithme de r√©gression",
                key="model_type_reg"
            )

            test_size_reg = st.slider(
                "üìä Taille test (%):",
                min_value=10, max_value=40, value=20, step=5,
                key="test_size_reg"
            ) / 100

        with col_conf2:
            handle_outliers = st.selectbox(
                "üìä Outliers:",
                options=["garder", "supprimer", "winsorize"],
                help="Traitement des valeurs extr√™mes",
                key="outliers_reg"
            )

            scale_method = st.selectbox(
                "üìè Normalisation:",
                options=["standard", "minmax", "robust", "aucune"],
                help="M√©thode de scaling",
                key="scale_reg"
            )

        with col_conf3:
            optimize_reg = st.checkbox("üîç Optimisation", value=True, key="optimize_reg")
            cv_reg = st.slider("üåÄ Validation crois√©e:", 2, 10, 5, key="cv_reg")
            remove_collinear = st.checkbox("üìâ Supprimer colin√©arit√©", value=True, key="collinear_reg")

        # Bouton d'entra√Ænement
        train_button_reg = st.button("üöÄ Entra√Æner mod√®le", type="primary", key="train_button_reg")

        if train_button_reg:
            if not target_col_reg or not selected_features_reg:
                st.error("‚ö†Ô∏è S√©lectionnez une variable cible et des features")
                st.stop()

            try:
                with st.spinner("üîÑ Pr√©paration des donn√©es en cours..."):
                    # Copie des donn√©es pour traitement
                    df_processed = df_final[[target_col_reg] + selected_features_reg].copy()

                    # Traitement des outliers
                    if handle_outliers == "supprimer":
                        numeric_cols = df_processed.select_dtypes(include=[np.number]).columns
                        initial_count = len(df_processed)
                        for col in numeric_cols:
                            if col != target_col_reg:  # Ne pas traiter la cible
                                Q1 = df_processed[col].quantile(0.25)
                                Q3 = df_processed[col].quantile(0.75)
                                IQR = Q3 - Q1
                                if IQR > 0:  # √âviter division par z√©ro
                                    lower = Q1 - 1.5 * IQR
                                    upper = Q3 + 1.5 * IQR
                                    df_processed = df_processed[
                                        (df_processed[col] >= lower) & (df_processed[col] <= upper)
                                        ]

                        removed_count = initial_count - len(df_processed)
                        if removed_count > 0:
                            st.info(
                                f"üìä {removed_count} outliers supprim√©s ({removed_count / initial_count * 100:.1f}%)")

                    # Pr√©paration avec la fonction utilitaire
                    scale_bool = scale_method != "aucune"

                    X_train, X_test, y_train, y_test, feature_names = prepare_data_for_ml(
                        df_processed,
                        target_col_reg,
                        selected_features_reg,
                        test_size=test_size_reg,
                        handle_missing="imputer_median",
                        scale_data=scale_bool,
                        task_type="regression"
                    )

                    # Afficher les informations
                    st.success("‚úÖ Donn√©es pr√©par√©es avec succ√®s")

                    col_info1, col_info2, col_info3, col_info4 = st.columns(4)
                    with col_info1:
                        st.metric("Train", f"{len(X_train):,}")
                    with col_info2:
                        st.metric("Test", f"{len(X_test):,}")
                    with col_info3:
                        st.metric("Features", X_train.shape[1])
                    with col_info4:
                        # Calcul de la corr√©lation moyenne
                        try:
                            corr_matrix = pd.concat([pd.DataFrame(X_train), pd.Series(y_train, name=target_col_reg)],
                                                    axis=1).corr()
                            avg_corr = corr_matrix[target_col_reg].abs().mean()
                            st.metric("Corr. moyenne", f"{avg_corr:.3f}")
                        except:
                            st.metric("Corr. moyenne", "N/A")

                # ENTRA√éNEMENT DU MOD√àLE DE R√âGRESSION
                with st.spinner(f"üìà Entra√Ænement du mod√®le {model_type_reg}..."):
                    # Import des mod√®les de r√©gression
                    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
                    from sklearn.linear_model import LinearRegression
                    from sklearn.metrics import (mean_squared_error, mean_absolute_error,
                                                 r2_score, mean_absolute_percentage_error)

                    # Initialisation du mod√®le selon le type
                    if model_type_reg == "random_forest":
                        model = RandomForestRegressor(
                            n_estimators=100,
                            max_depth=10,
                            min_samples_split=5,
                            random_state=42,
                            n_jobs=-1
                        )

                    elif model_type_reg == "xgboost":
                        try:
                            from xgboost import XGBRegressor

                            model = XGBRegressor(
                                n_estimators=100,
                                max_depth=6,
                                learning_rate=0.1,
                                random_state=42
                            )
                        except ImportError:
                            st.warning("XGBoost non install√©, utilisation de Random Forest")
                            model = RandomForestRegressor(n_estimators=100, random_state=42)

                    elif model_type_reg == "gradient_boosting":
                        model = GradientBoostingRegressor(
                            n_estimators=100,
                            learning_rate=0.1,
                            max_depth=3,
                            random_state=42
                        )

                    elif model_type_reg == "linear_regression":
                        model = LinearRegression()

                    else:
                        model = RandomForestRegressor(n_estimators=100, random_state=42)

                    # Optimisation des hyperparam√®tres si demand√©e
                    if optimize_reg:
                        with st.spinner("üîç Optimisation des hyperparam√®tres..."):
                            from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

                            if model_type_reg == "random_forest":
                                param_grid = {
                                    'n_estimators': [50, 100, 200],
                                    'max_depth': [5, 10, 15, None],
                                    'min_samples_split': [2, 5, 10]
                                }
                                search = GridSearchCV(
                                    model, param_grid,
                                    cv=min(cv_reg, 5),
                                    scoring='r2',
                                    n_jobs=-1,
                                    verbose=0
                                )

                            elif model_type_reg == "xgboost":
                                param_grid = {
                                    'n_estimators': [50, 100, 150],
                                    'max_depth': [3, 6, 9],
                                    'learning_rate': [0.01, 0.1, 0.3]
                                }
                                search = RandomizedSearchCV(
                                    model, param_grid,
                                    n_iter=10,
                                    cv=min(cv_reg, 5),
                                    scoring='r2',
                                    n_jobs=-1,
                                    random_state=42,
                                    verbose=0
                                )

                            elif model_type_reg == "linear_regression":
                                param_grid = {
                                    'fit_intercept': [True, False]
                                }
                                search = GridSearchCV(
                                    model, param_grid,
                                    cv=min(cv_reg, 5),
                                    scoring='r2',
                                    n_jobs=-1,
                                    verbose=0
                                )

                            else:
                                search = None

                            if search is not None:
                                search.fit(X_train, y_train)
                                model = search.best_estimator_
                                st.success(f"‚úÖ Meilleurs param√®tres: {search.best_params_}")

                    # Entra√Ænement du mod√®le
                    model.fit(X_train, y_train)

                    # Pr√©dictions
                    y_pred = model.predict(X_test)

                    # Sauvegarde dans session state
                    st.session_state.regression_model = {
                        'model': model,
                        'features': feature_names,
                        'target': target_col_reg,
                        'model_type': model_type_reg,
                        'X_train': X_train,
                        'X_test': X_test,
                        'y_train': y_train,
                        'y_test': y_test,
                        'y_pred': y_pred
                    }

                st.success("‚úÖ Mod√®le de r√©gression entra√Æn√© avec succ√®s !")

                # ============================================
                # AFFICHAGE DES R√âSULTATS
                # ============================================

                # 1. M√©triques de performance
                st.subheader("üìä Performance du mod√®le")

                # Calcul des m√©triques
                mse = mean_squared_error(y_test, y_pred)
                rmse = np.sqrt(mse)
                mae = mean_absolute_error(y_test, y_pred)
                r2 = r2_score(y_test, y_pred)

                try:
                    mape = mean_absolute_percentage_error(y_test, y_pred) * 100
                except:
                    mape = None

                # Affichage des m√©triques
                col_met1, col_met2, col_met3, col_met4 = st.columns(4)
                with col_met1:
                    st.metric("R¬≤ Score", f"{r2:.3f}")
                with col_met2:
                    st.metric("RMSE", f"{rmse:.3f}")
                with col_met3:
                    st.metric("MAE", f"{mae:.3f}")
                with col_met4:
                    if mape is not None:
                        st.metric("MAPE", f"{mape:.1f}%")
                    else:
                        st.metric("MAPE", "N/A")

                # 2. Graphique des pr√©dictions vs vraies valeurs
                st.subheader("üìà Pr√©dictions vs Vraies valeurs")

                # Cr√©ation du graphique
                fig_scatter = go.Figure()

                # Nuage de points des pr√©dictions
                fig_scatter.add_trace(go.Scatter(
                    x=y_test,
                    y=y_pred,
                    mode='markers',
                    name='Pr√©dictions',
                    marker=dict(
                        size=8,
                        color='blue',
                        opacity=0.6,
                        line=dict(width=1, color='DarkSlateGrey')
                    ),
                    hovertemplate='<b>Vraie valeur</b>: %{x:.2f}<br><b>Pr√©diction</b>: %{y:.2f}<br><b>Erreur</b>: %{customdata:.2f}<extra></extra>',
                    customdata=np.abs(y_test - y_pred)
                ))

                # Ligne de parfaite pr√©diction
                min_val = min(y_test.min(), y_pred.min())
                max_val = max(y_test.max(), y_pred.max())
                fig_scatter.add_trace(go.Scatter(
                    x=[min_val, max_val],
                    y=[min_val, max_val],
                    mode='lines',
                    name='Parfait',
                    line=dict(color='red', width=2, dash='dash')
                ))

                fig_scatter.update_layout(
                    title='Comparaison des pr√©dictions avec les vraies valeurs',
                    xaxis_title='Vraies valeurs',
                    yaxis_title='Pr√©dictions',
                    showlegend=True,
                    width=800,
                    height=600
                )

                st.plotly_chart(fig_scatter, use_container_width=True)

                # 3. Distribution des erreurs
                st.subheader("üìä Distribution des erreurs")

                errors = y_test - y_pred

                col_err1, col_err2 = st.columns(2)

                with col_err1:
                    # Histogramme des erreurs
                    fig_err_hist = px.histogram(
                        x=errors,
                        nbins=50,
                        title="Distribution des erreurs",
                        labels={'x': 'Erreur', 'y': 'Fr√©quence'},
                        color_discrete_sequence=['#FF6B6B']
                    )
                    fig_err_hist.add_vline(x=0, line_dash="dash", line_color="red")
                    st.plotly_chart(fig_err_hist, use_container_width=True)

                with col_err2:
                    # QQ plot des erreurs
                    from scipy import stats

                    qq_data = stats.probplot(errors, dist="norm")
                    x = qq_data[0][0]
                    y = qq_data[0][1]

                    fig_qq = go.Figure()
                    fig_qq.add_trace(go.Scatter(
                        x=x, y=y,
                        mode='markers',
                        name='Erreurs',
                        marker=dict(color='blue', size=6)
                    ))

                    # Ligne de r√©f√©rence
                    fig_qq.add_trace(go.Scatter(
                        x=[x.min(), x.max()],
                        y=[x.min(), x.max()],
                        mode='lines',
                        name='Normale',
                        line=dict(color='red', dash='dash')
                    ))

                    fig_qq.update_layout(
                        title="QQ Plot des erreurs",
                        xaxis_title="Quantiles th√©oriques",
                        yaxis_title="Quantiles observ√©s"
                    )
                    st.plotly_chart(fig_qq, use_container_width=True)

                # 4. Importance des features (si disponible)
                if hasattr(model, 'feature_importances_'):
                    st.subheader("üéØ Importance des features")

                    # Cr√©ation du dataframe d'importance
                    importance_df = pd.DataFrame({
                        'Feature': feature_names[:len(model.feature_importances_)],
                        'Importance': model.feature_importances_
                    }).sort_values('Importance', ascending=False)

                    # Graphique des top 15 features
                    fig_importance = px.bar(
                        importance_df.head(15),
                        x='Importance',
                        y='Feature',
                        orientation='h',
                        title='Top 15 des features les plus importantes',
                        color='Importance',
                        color_continuous_scale='Viridis'
                    )
                    fig_importance.update_layout(yaxis={'categoryorder': 'total ascending'})
                    st.plotly_chart(fig_importance, use_container_width=True)

                # 5. Pr√©dictions d√©taill√©es
                st.subheader("üîÆ Pr√©dictions d√©taill√©es")

                results_df = pd.DataFrame({
                    'Vraie_valeur': y_test.values,
                    'Pr√©diction': y_pred,
                    'Erreur': errors,
                    'Erreur_abs': np.abs(errors),
                    'Erreur_pourcentage': np.abs(errors / y_test * 100) if (y_test != 0).all() else np.nan
                })

                # Affichage des pr√©dictions
                with st.expander("üìÑ Voir les pr√©dictions"):
                    st.dataframe(
                        results_df.sort_values('Erreur_abs', ascending=False).head(50)
                        .style.format({
                            'Vraie_valeur': '{:.2f}',
                            'Pr√©diction': '{:.2f}',
                            'Erreur': '{:.2f}',
                            'Erreur_abs': '{:.2f}',
                            'Erreur_pourcentage': '{:.1f}%'
                        })
                        .background_gradient(subset=['Erreur_abs'], cmap='Reds'),
                        use_container_width=True
                    )

                # T√©l√©chargement des pr√©dictions
                csv = results_df.to_csv(index=False)
                st.download_button(
                    label="üì• T√©l√©charger toutes les pr√©dictions",
                    data=csv,
                    file_name=f"predictions_regression_{target_col_reg}_{model_type_reg}.csv",
                    mime="text/csv",
                    key="download_preds_reg"
                )

                # 6. Export du mod√®le
                st.markdown("---")
                st.subheader("üíæ Export du mod√®le")

                col_exp1, col_exp2 = st.columns(2)

                with col_exp1:
                    if st.button("üíæ Sauvegarder le mod√®le", key="save_model_reg"):
                        import joblib
                        import os

                        os.makedirs("models", exist_ok=True)
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        model_path = f"models/regression_{target_col_reg}_{model_type_reg}_{timestamp}.pkl"

                        model_data = {
                            'model': model,
                            'features': feature_names,
                            'target': target_col_reg,
                            'model_type': model_type_reg,
                            'r2_score': r2,
                            'rmse': rmse,
                            'metadata': {
                                'training_date': timestamp,
                                'n_samples_train': len(X_train),
                                'n_samples_test': len(X_test),
                                'n_features': len(feature_names),
                                'test_size': test_size_reg
                            }
                        }

                        joblib.dump(model_data, model_path)
                        st.success(f"‚úÖ Mod√®le sauvegard√©: `{model_path}`")

                with col_exp2:
                    # Code pour reproduire le mod√®le
                    with st.expander("üìù Code de reproduction"):
                        st.code(f"""
    # Code pour reproduire le mod√®le {model_type_reg}
    import pandas as pd
    import numpy as np
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler

    # Charger les donn√©es
    # df = pd.read_csv('votre_fichier.csv')

    # S√©lectionner les m√™mes variables
    X = df[{selected_features_reg}]
    y = df['{target_col_reg}']

    # Encodage one-hot pour les variables cat√©gorielles
    X = pd.get_dummies(X, drop_first=True)

    # Imputation des valeurs manquantes
    from sklearn.impute import SimpleImputer
    imputer = SimpleImputer(strategy='median')
    X = imputer.fit_transform(X)

    # Normalisation
    scaler = StandardScaler()
    X = scaler.fit_transform(X)

    # Split des donn√©es
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size={test_size_reg}, random_state=42
    )

    # Entra√Æner le mod√®le
    from sklearn.ensemble import RandomForestRegressor
    model = RandomForestRegressor(
        n_estimators=100,
        max_depth=10,
        random_state=42
    )
    model.fit(X_train, y_train)

    # √âvaluation
    y_pred = model.predict(X_test)
    r2 = model.score(X_test, y_test)
    print(f"R¬≤ Score: {{r2:.3f}}")
                        """)

            except Exception as e:
                st.error(f"‚ùå Erreur: {str(e)}")
                st.code(traceback.format_exc())
# ============================================================
# üìÑ Rapport Intelligent
# ============================================================
elif page == " üìÑ Rapport Intelligent":
    st.header(" G√©n√©ration de rapport IA")

    if not REPORT_ENGINE_AVAILABLE:
        st.error(" Le module report_engine n'est pas disponible. Installez les d√©pendances n√©cessaires.")
        st.info("""
        **D√©pendances n√©cessaires :**
        ```bash
        pip install reportlab python-docx markdown
        ```
        """)
        st.stop()

    if st.session_state.report_engine is None:
        try:
            st.session_state.report_engine = ReportEngine(ai_client=None)
        except Exception as e:
            st.error(f" Impossible d'initialiser le moteur de rapport: {e}")
            st.stop()

    if hasattr(st.session_state, 'using_mateur') and st.session_state.using_mateur:
        st.info(" **Moteur : Mateur AI** - Analyse 100% locale, aucune donn√©e externe")
    elif st.session_state.openai_client is not None:
        st.info(" **T√©l√©chargez** le rapport en un clique !")
    else:
        st.info("**Moteur : Local** - G√©n√©ration basique")

    if not st.session_state.data_ready:
        st.warning(" Veuillez charger les donn√©es")
        st.stop()

    title = st.text_input(
        "Titre du rapport",
        "Rapport d'analyse ‚Äì Assurance Automobile"
    )

    audience = st.selectbox(
        "Public cible",
        ["Direction g√©n√©rale", "Direction m√©tier", "√âquipe data", "Audit", "Comit√© de pilotage"]
    )

    sections = st.multiselect(
        "Sections √† inclure",
        [
            "executive_summary",
            "data_context",
            "data_quality",
            "statistics",
            "models",
            "scoring",
            "insights",
            "recommendations",
            "limitations",
            "annexes"
        ],
        default=["executive_summary", "scoring", "recommendations"]
    )

    custom_instructions = st.text_area(
        "Instructions personnalis√©es",
        placeholder="Ex : Insister sur la rentabilit√©, mentionner les risques r√©glementaires, proposer un plan d'action concret...",
        height=100
    )

    st.subheader(" Options d'export")
    col_export1, col_export2, col_export3, col_export4 = st.columns(4)
    with col_export1:
        export_md = st.checkbox("Markdown (.md)", value=True)
    with col_export2:
        export_pdf = st.checkbox("PDF (.pdf)", value=True)
    with col_export3:
        export_word = st.checkbox("Word (.docx)", value=True)
    with col_export4:
        export_html = st.checkbox("HTML (.html)", value=True)

    if st.button(" G√©n√©rer le rapport complet", type="primary"):
        with st.spinner("G√©n√©ration du rapport en cours..."):
            try:
                data_summary = {
                    "rows": st.session_state.dataframe.shape[0],
                    "columns": st.session_state.dataframe.shape[1],
                    "key_variables": list(st.session_state.dataframe.columns[:10]),
                    "completeness": round((1 - st.session_state.dataframe.isna().sum().sum() /
                                           (st.session_state.dataframe.shape[0] * st.session_state.dataframe.shape[
                                               1])) * 100, 1)
                }

                analysis_summary = "Analyse descriptive + scoring client"

                if st.session_state.scored_clients is not None:
                    scored_clients = st.session_state.scored_clients
                    analysis_summary += f"\n- Clients analys√©s : {len(scored_clients):,}"
                    analysis_summary += f"\n- Score risque moyen : {scored_clients['score_risque'].mean():.1f}/100"
                    high_risk = (scored_clients['niveau_risque'] == '√âlev√©').sum()
                    analysis_summary += f"\n- Clients √† risque √©lev√© : {high_risk}"
                    data_summary["high_risk_count"] = high_risk

                insights = None
                if st.session_state.scored_clients is not None:
                    try:
                        from modules.insight_engine import InsightEngine

                        insight_engine = InsightEngine()
                        insights = insight_engine.generate_insights(st.session_state.scored_clients)
                    except ImportError as e:
                        insights = ["Insights sur les risques clients disponibles dans l'onglet 'Insights Avanc√©s'"]
                    except Exception as e:
                        insights = [f"Insights : {str(e)}"]

                report_md = st.session_state.report_engine.generate_report(
                    title=title,
                    audience=audience,
                    sections=sections,
                    data_summary=data_summary,
                    analysis_summary=analysis_summary,
                    model_results=None,
                    insights=insights,
                    custom_instructions=custom_instructions,
                    detail_level=4
                )

                st.session_state.generated_report_md = report_md
                st.success(" Rapport markdown g√©n√©r√© avec succ√®s!")

                if export_pdf:
                    with st.spinner("G√©n√©ration du PDF..."):
                        try:
                            pdf_buffer = st.session_state.report_engine.to_pdf(report_md, title)
                            st.session_state.generated_report_pdf = pdf_buffer.getvalue()
                            st.success(" PDF g√©n√©r√© avec succ√®s!")
                        except Exception as e:
                            st.warning(f" PDF non g√©n√©r√©: {str(e)}")
                            st.session_state.generated_report_pdf = None

                if export_word:
                    with st.spinner("G√©n√©ration du document Word..."):
                        try:
                            word_buffer = st.session_state.report_engine.to_word(report_md, title)
                            st.session_state.generated_report_word = word_buffer.getvalue()
                            st.success(" Document Word g√©n√©r√© avec succ√®s!")
                        except Exception as e:
                            st.warning(f" Word non g√©n√©r√©: {str(e)}")
                            st.session_state.generated_report_word = None

                if export_html:
                    try:
                        html_report = st.session_state.report_engine.to_html(report_md)
                        st.session_state.generated_report_html = html_report
                        st.success(" HTML g√©n√©r√© avec succ√®s!")
                    except Exception as e:
                        st.warning(f" HTML non g√©n√©r√©: {str(e)}")
                        st.session_state.generated_report_html = None

            except Exception as e:
                st.error(f" Erreur lors de la g√©n√©ration du rapport: {str(e)}")
                st.code(traceback.format_exc())

    if hasattr(st.session_state, 'generated_report_md') and st.session_state.generated_report_md:
        st.markdown("---")
        st.subheader(" Aper√ßu du rapport")

        with st.expander(" Voir le rapport complet", expanded=False):
            st.markdown(st.session_state.generated_report_md)

        st.subheader(" T√©l√©chargements")
        cols = st.columns(4)

        with cols[0]:
            filename = f"rapport_{datetime.now().strftime('%Y%m%d_%H%M')}"
            st.download_button(
                label=" Markdown",
                data=st.session_state.generated_report_md,
                file_name=f"{filename}.md",
                mime="text/markdown",
                help="Format texte avec mise en forme"
            )

        if hasattr(st.session_state, 'generated_report_pdf') and st.session_state.generated_report_pdf:
            with cols[1]:
                st.download_button(
                    label=" PDF",
                    data=st.session_state.generated_report_pdf,
                    file_name=f"{filename}.pdf",
                    mime="application/pdf",
                    help="Document format√© pour impression"
                )

        if hasattr(st.session_state, 'generated_report_word') and st.session_state.generated_report_word:
            with cols[2]:
                st.download_button(
                    label=" Word",
                    data=st.session_state.generated_report_word,
                    file_name=f"{filename}.docx",
                    mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                    help="Document √©ditable Microsoft Word"
                )

        if hasattr(st.session_state, 'generated_report_html') and st.session_state.generated_report_html:
            with cols[3]:
                st.download_button(
                    label=" HTML",
                    data=st.session_state.generated_report_html,
                    file_name=f"{filename}.html",
                    mime="text/html",
                    help="Page web autonome"
                )

        st.info("""
        **Formats disponibles :**
        - **Markdown** : Format texte simple, √©ditable (.Rmd)
        - **PDF** : Document format√© pour impression et partage
        - **Word** : Document √©ditable (Word)
        - **HTML** : Page web 
        """)

# ============================================================
# üè¢ PAGE "√Ä PROPOS"
# ============================================================
elif page == "üè¢ √Ä Propos":
    st.header("üè¢ √Ä Propos de LIK Insurance Analyst")

    # Banni√®re avec logo et message d'accroche
    st.markdown("""
    <div style='text-align: center; padding: 30px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; border-radius: 15px; margin-bottom: 30px;'>
        <h1 style='margin-bottom: 10px;'>LIK Insurance Analyst</h1>
        <h3 style='font-weight: normal; margin-top: 0;'>Intelligence Artificielle au Service de l'Assurance</h3>
    </div>
    """, unsafe_allow_html=True)

    # Pr√©sentation en 2 colonnes
    col_pres1, col_pres2 = st.columns([2, 1])

    with col_pres1:
        st.markdown("""
        ### üéØ Notre Mission

        **Transformer les donn√©es brutes en d√©cisions strat√©giques**  

        LIK Insurance Analyst est une plateforme innovante qui combine l'expertise du secteur de l'assurance avec les derni√®res avanc√©es en intelligence artificielle. Notre solution permet aux assureurs d'optimiser leur gestion des risques, d'am√©liorer leur rentabilit√© et d'offrir une exp√©rience client exceptionnelle.
        """)

        st.markdown("""
        ### üöÄ Notre Vision

        **Devenir le partenaire privil√©gi√© des assureurs dans leur transformation digitale**  

        Nous aspirons √† d√©mocratiser l'acc√®s aux technologies d'IA avanc√©es pour tous les acteurs du secteur de l'assurance, des petites mutuelles aux grands groupes internationaux.
        """)

    with col_pres2:
        # Statistiques cl√©s
        st.markdown("""
        <div style='background-color: #f8f9fa; padding: 20px; border-radius: 10px; text-align: center;'>
            <h3 style='color: #1E3A8A;'>üìä En Chiffres</h3>
            <div style='font-size: 36px; font-weight: bold; color: #667eea;'>100%</div>
            <p>S√©curit√© des donn√©es</p>
            <div style='font-size: 36px; font-weight: bold; color: #667eea;'>+30%</div>
            <p>Pr√©cision des mod√®les</p>
            <div style='font-size: 36px; font-weight: bold; color: #667eea;'>24/7</div>
            <p>Support disponible</p>
        </div>
        """, unsafe_allow_html=True)

    st.markdown("---")

    # Nos Valeurs
    st.subheader("üíé Nos Valeurs Fondamentales")

    valeurs = [
        {
            "emoji": "üîí",
            "titre": "Confidentialit√©",
            "description": "Vos donn√©es restent 100% locales et s√©curis√©es",
            "couleur": "#e8f4fd"
        },
        {
            "emoji": "‚ö°",
            "titre": "Innovation",
            "description": "Technologies de pointe en IA et Machine Learning",
            "couleur": "#f0f9ff"
        },
        {
            "emoji": "üéØ",
            "titre": "Pr√©cision",
            "description": "Analyses scientifiques et r√©sultats fiables",
            "couleur": "#f8f9fa"
        },
        {
            "emoji": "ü§ù",
            "titre": "Collaboration",
            "description": "Partage d'expertise avec nos partenaires",
            "couleur": "#e8f7ec"
        },
        {
            "emoji": "üìà",
            "titre": "Performance",
            "description": "Optimisation continue des r√©sultats",
            "couleur": "#fff3cd"
        },
        {
            "emoji": "üåç",
            "titre": "Accessibilit√©",
            "description": "Solutions adapt√©es √† tous les budgets",
            "couleur": "#d1ecf1"
        }
    ]

    cols = st.columns(3)
    for i, valeur in enumerate(valeurs):
        with cols[i % 3]:
            st.markdown(f"""
            <div style='background-color: {valeur['couleur']}; padding: 20px; border-radius: 10px; text-align: center; margin-bottom: 15px; min-height: 150px; display: flex; flex-direction: column; justify-content: center;'>
                <div style='font-size: 40px; margin-bottom: 10px;'>{valeur['emoji']}</div>
                <h4 style='margin: 0 0 10px 0; color: #1E3A8A;'>{valeur['titre']}</h4>
                <p style='margin: 0; font-size: 14px;'>{valeur['description']}</p>
            </div>
            """, unsafe_allow_html=True)

    st.markdown("---")

    # Technologies et M√©thodologie
    st.subheader("üõ†Ô∏è Notre Stack Technologique")

    tech_cols = st.columns(3)

    with tech_cols[0]:
        st.markdown("""
        <div style='background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); color: white; padding: 20px; border-radius: 10px;'>
            <h4 style='margin-top: 0;'>ü§ñ Intelligence Artificielle</h4>
            <p>‚Ä¢ OpenAI GPT-4/GPT-3.5</p>
            <p>‚Ä¢ Scikit-learn & TensorFlow</p>
            <p>‚Ä¢ XGBoost & CatBoost</p>
            <p>‚Ä¢ Transformers NLP</p>
        </div>
        """, unsafe_allow_html=True)

    with tech_cols[1]:
        st.markdown("""
        <div style='background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%); color: white; padding: 20px; border-radius: 10px;'>
            <h4 style='margin-top: 0;'>üìä Data Science</h4>
            <p>‚Ä¢ Pandas & NumPy</p>
            <p>‚Ä¢ Plotly & Altair</p>
            <p>‚Ä¢ Statsmodels</p>
            <p>‚Ä¢ Prophet & ARIMA</p>
        </div>
        """, unsafe_allow_html=True)

    with tech_cols[2]:
        st.markdown("""
        <div style='background: linear-gradient(135deg, #43e97b 0%, #38f9d7 100%); color: white; padding: 20px; border-radius: 10px;'>
            <h4 style='margin-top: 0;'>üíª D√©veloppement</h4>
            <p>‚Ä¢ Streamlit & FastAPI</p>
            <p>‚Ä¢ Python 3.11+</p>
            <p>‚Ä¢ Docker & Kubernetes</p>
            <p>‚Ä¢ Git & CI/CD</p>
        </div>
        """, unsafe_allow_html=True)

    st.markdown("---")

    # Notre Approche
    st.subheader("üî¨ Notre Approche Scientifique")

    approche_steps = [
        ("1Ô∏è‚É£", "Analyse Exploratoire", "Compr√©hension approfondie de vos donn√©es et du contexte m√©tier"),
        ("2Ô∏è‚É£", "Pr√©paration Scientifique", "Nettoyage et transformation rigoureuse des donn√©es"),
        ("3Ô∏è‚É£", "Mod√©lisation Avanc√©e", "D√©veloppement de mod√®les pr√©dictifs adapt√©s"),
        ("4Ô∏è‚É£", "Validation Rigoureuse", "Tests statistiques et validation m√©tier"),
        ("5Ô∏è‚É£", "D√©ploiement S√©curis√©", "Int√©gration dans vos processus existants"),
        ("6Ô∏è‚É£", "Suivi Continu", "Monitoring et am√©lioration continue")
    ]

    for emoji, titre, description in approche_steps:
        st.markdown(f"""
        <div style='display: flex; align-items: flex-start; margin-bottom: 15px; padding: 15px; background-color: #f8f9fa; border-radius: 10px;'>
            <div style='font-size: 30px; margin-right: 15px;'>{emoji}</div>
            <div>
                <h4 style='margin: 0 0 5px 0; color: #1E3A8A;'>{titre}</h4>
                <p style='margin: 0; font-size: 14px;'>{description}</p>
            </div>
        </div>
        """, unsafe_allow_html=True)

    st.markdown("---")

    # Section S√©curit√©
    st.subheader("üîí Notre Engagement S√©curit√©")

    sec_cols = st.columns(2)

    with sec_cols[0]:
        st.markdown("""
        <div style='background-color: #d4edda; padding: 20px; border-radius: 10px;'>
            <h4 style='color: #155724; margin-top: 0;'>‚úÖ Ce que nous garantissons</h4>
            <p>‚Ä¢ Donn√©es 100% locales et s√©curis√©es</p>
            <p>‚Ä¢ Conformit√© RGPD et r√©glementations locales</p>
            <p>‚Ä¢ Chiffrement AES-256 pour toutes les donn√©es</p>
            <p>‚Ä¢ Authentification multi-facteurs</p>
            <p>‚Ä¢ Sauvegardes r√©guli√®res et chiffr√©es</p>
        </div>
        """, unsafe_allow_html=True)

    with sec_cols[1]:
        st.markdown("""
        <div style='background-color: #f8d7da; padding: 20px; border-radius: 10px;'>
            <h4 style='color: #721c24; margin-top: 0;'>‚ùå Ce que nous ne faisons pas</h4>
            <p>‚Ä¢ Vendre ou partager vos donn√©es</p>
            <p>‚Ä¢ Envoyer vos donn√©es vers le cloud sans consentement</p>
            <p>‚Ä¢ Stocker des donn√©es sensibles non chiffr√©es</p>
            <p>‚Ä¢ Utiliser des logiciels non s√©curis√©s</p>
            <p>‚Ä¢ Acc√®s non autoris√© √† vos syst√®mes</p>
        </div>
        """, unsafe_allow_html=True)

    st.markdown("---")

    # Section Partenariat DXC
    st.subheader("ü§ù Notre Partenariat Strat√©gique")

    st.markdown("""
    <div style='text-align: center; padding: 30px; background: linear-gradient(135deg, #1E3A8A 0%, #3B82F6 100%); color: white; border-radius: 15px; margin: 20px 0;'>
        <h2 style='margin-bottom: 10px;'>üíº Partenariat avec DXC Technology</h2>
        <p style='font-size: 18px; margin-bottom: 20px;'>Expertise globale en transformation digitale</p>
        <a href='https://dxc.com/' target='_blank' style='display: inline-block; background-color: white; color: #1E3A8A; padding: 12px 30px; text-decoration: none; border-radius: 25px; font-weight: bold; margin-top: 10px;'>
            üåê D√©couvrir DXC Technology
        </a>
    </div>
    """, unsafe_allow_html=True)

    st.markdown("""
    ### üí° Pourquoi ce partenariat ?

    Notre collaboration avec DXC Technology nous permet de :

    - **B√©n√©ficier d'une expertise internationale** en transformation digitale
    - **Acc√©der aux derni√®res technologies** en mati√®re de cloud et de s√©curit√©
    - **√âlargir notre portefeuille de solutions** pour mieux r√©pondre √† vos besoins
    - **Assurer un support technique** de niveau entreprise
    - **Maintenir notre avance technologique** gr√¢ce √† la R&D commune
    """)

    st.markdown("---")

    # Contact
    st.subheader("üìû Contactez-nous")

    contact_cols = st.columns(3)

    with contact_cols[0]:
        st.markdown("""
        <div style='text-align: center; padding: 20px; background-color: #e8f4fd; border-radius: 10px;'>
            <div style='font-size: 40px; margin-bottom: 10px;'>üìß</div>
            <h4>Email</h4>
            <p style='margin: 5px 0;'>contact@lik-insurance.ma</p>
            <p style='margin: 5px 0;'>support@lik-insurance.ma</p>
        </div>
        """, unsafe_allow_html=True)

    with contact_cols[1]:
        st.markdown("""
        <div style='text-align: center; padding: 20px; background-color: #f0f9ff; border-radius: 10px;'>
            <div style='font-size: 40px; margin-bottom: 10px;'>üì±</div>
            <h4>T√©l√©phone</h4>
            <p style='margin: 5px 0;'>+212 5 XX XX XX XX</p>
            <p style='margin: 5px 0;'>Lundi - Vendredi</p>
            <p style='margin: 5px 0;'>9h00 - 18h00</p>
        </div>
        """, unsafe_allow_html=True)

    with contact_cols[2]:
        st.markdown("""
        <div style='text-align: center; padding: 20px; background-color: #f8f9fa; border-radius: 10px;'>
            <div style='font-size: 40px; margin-bottom: 10px;'>üè¢</div>
            <h4>Adresse</h4>
            <p style='margin: 5px 0;'>Tour Hassan Tower</p>
            <p style='margin: 5px 0;'>Bureau 1504, 15√®me √©tage</p>
            <p style='margin: 5px 0;'>Rabat, Maroc</p>
        </div>
        """, unsafe_allow_html=True)
#=================================================
#FOOTER AM√âLIOR√â
#============================================================

